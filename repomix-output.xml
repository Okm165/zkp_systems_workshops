This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    spelling.yaml
0_zkp_architecture/
  src/
    main.rs
  Cargo.toml
  README.md
1_mathematical_toolkit/
  src/
    main.rs
    tests.rs
  Cargo.toml
  README.md
2_fast_polynomial_arithmetic/
  benches/
    lines.svg
    polynomial_multiplication.rs
  src/
    lib.rs
    main.rs
  Cargo.toml
  README.md
3_polynomial_commitment_scheme/
  src/
    error.rs
    main.rs
    prover.rs
    types.rs
    verifier.rs
  Cargo.toml
  README.md
4_air_constraints_design/
  src/
    arithmetization.rs
    composition.rs
    deep_composition.rs
    main.rs
    trace.rs
  Cargo.toml
  README.md
.gitignore
Cargo.toml
README.md
rust-toolchain.toml
rustfmt.toml
typos.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/spelling.yaml">
name: Spelling

permissions:
  contents: read

on: [pull_request]

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always
  CLICOLOR: 1

concurrency:
  group: "${{ github.workflow }}-${{ github.ref }}"
  cancel-in-progress: true

jobs:
  spelling:
    name: Spell Check with Typos
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Actions Repository
      uses: actions/checkout@v4
    - name: Spell Check Repo
      uses: crate-ci/typos@master
</file>

<file path="0_zkp_architecture/src/main.rs">
fn main() {
    println!("Hello, world!");
}
</file>

<file path="0_zkp_architecture/Cargo.toml">
[package]
name = "zkp_architecture"
edition.workspace = true
version.workspace = true
readme = "README.md"

[dependencies]
</file>

<file path="1_mathematical_toolkit/src/main.rs">
pub mod tests;

// Finite Field Arithmetic over GF(p)
#[derive(Debug, Clone)]
pub struct PrimeField {
    pub p: u64,
}

impl PrimeField {
    pub fn new(p: u64) -> Self {
        assert!(Self::is_prime(p), "p must be prime");
        Self { p }
    }

    pub fn add(&self, a: u64, b: u64) -> u64 {
        (a + b) % self.p
    }

    pub fn sub(&self, a: u64, b: u64) -> u64 {
        (a + self.p - b) % self.p
    }

    pub fn mul(&self, a: u64, b: u64) -> u64 {
        (a * b) % self.p
    }

    pub fn neg(&self, a: u64) -> u64 {
        (self.p - a % self.p) % self.p
    }

    pub fn inv(&self, a: u64) -> u64 {
        assert!(a != 0, "No inverse for 0");
        Self::mod_pow(a, self.p - 2, self.p)
    }

    pub fn div(&self, a: u64, b: u64) -> u64 {
        self.mul(a, self.inv(b))
    }

    pub fn is_prime(n: u64) -> bool {
        if n <= 1 {
            return false;
        }
        if n <= 3 {
            return true;
        }
        if n % 2 == 0 || n % 3 == 0 {
            return false;
        }
        let mut i = 5;
        while i * i <= n {
            if n % i == 0 || n % (i + 2) == 0 {
                return false;
            }
            i += 6;
        }
        true
    }

    pub fn mod_pow(mut base: u64, mut exp: u64, modulus: u64) -> u64 {
        let mut result = 1;
        base %= modulus;
        while exp > 0 {
            if exp % 2 == 1 {
                result = (result * base) % modulus;
            }
            base = (base * base) % modulus;
            exp /= 2;
        }
        result
    }
}

// Polynomial over GF(p)
#[derive(Debug, Clone)]
pub struct Polynomial {
    coeffs: Vec<u64>,
    field: PrimeField,
}

impl Polynomial {
    pub fn new(coeffs: Vec<u64>, field: PrimeField) -> Self {
        let coeffs = coeffs.into_iter().map(|c| c % field.p).collect();
        Self { coeffs, field }
    }

    pub fn evaluate(&self, x: u64) -> u64 {
        let mut result = 0;
        for (power, &coeff) in self.coeffs.iter().enumerate() {
            let x_pow = PrimeField::mod_pow(x, power as u64, self.field.p);
            let term = self.field.mul(coeff, x_pow);
            result = self.field.add(result, term);
        }
        result
    }
}

fn main() {
    let f = PrimeField::new(7);
    println!("5 + 3 mod 7 = {}", f.add(5, 3)); // Output: 1
    println!("3 / 4 mod 7 = {}", f.div(3, 4)); // Output: 6

    let poly = Polynomial::new(vec![1, 0, 1], f.clone()); // x^2 + 1
    println!("P(0) = {}", poly.evaluate(0)); // 1
    println!("P(2) = {}", poly.evaluate(2)); // 2^2 + 1 = 5
}
</file>

<file path="2_fast_polynomial_arithmetic/src/lib.rs">
//! This crate provides functions for polynomial multiplication using
//! both the Fast Fourier Transform (FFT) algorithm and a naive O(N^2) approach.
//! It leverages the `lambdaworks_math` library for field arithmetic and FFT primitives.

use lambdaworks_math::fft::cpu::bit_reversing::in_place_bit_reverse_permute;
use lambdaworks_math::fft::cpu::fft::in_place_nr_2radix_fft;
use lambdaworks_math::field::element::FieldElement;
use lambdaworks_math::field::fields::fft_friendly::babybear_u32::Babybear31PrimeField;
use lambdaworks_math::polynomial::Polynomial;

// Type aliases for convenience, specifying the field to be Babybear31PrimeField.
type F = Babybear31PrimeField;
type FE = FieldElement<F>;

/// Multiplies two polynomials using the Fast Fourier Transform (FFT) algorithm.
///
/// This function performs polynomial multiplication in O(N log N) time, where N is
/// the size of the evaluation domain (a power of 2).
///
/// # Arguments
/// * `p1` - The first polynomial.
/// * `p2` - The second polynomial.
/// * `n` - The size of the FFT domain. Must be a power of 2 and `n >= degree(p1) + degree(p2) + 1`.
/// * `twiddles` - Precomputed bit-reversed roots of unity for the forward FFT.
/// * `inv_twiddles` - Precomputed bit-reversed inverse roots of unity for the Inverse FFT.
///
/// # Returns
/// A new `Polynomial` representing the product `p1 * p2`.
///
/// # Panics
/// This function does not explicitly panic, but relies on the correctness of `lambdaworks_math`
/// functions. Incorrect `n` or precomputed twiddles may lead to incorrect results.
pub fn multiply_polynomials_fft(
    p1: &Polynomial<FE>,
    p2: &Polynomial<FE>,
    n: usize,
    twiddles: &[FieldElement<F>],
    inv_twiddles: &[FieldElement<F>],
) -> Polynomial<FE> {
    // 1. Pad coefficients to match the FFT domain size `n`.
    // The FFT algorithm requires the input vectors to have a length equal to the domain size.
    let mut p1_coeffs = p1.coefficients.to_vec();
    p1_coeffs.resize(n, FE::zero()); // Pad with zeros.

    let mut p2_coeffs = p2.coefficients.to_vec();
    p2_coeffs.resize(n, FE::zero()); // Pad with zeros.

    // 2. Perform Fast Fourier Transform (FFT) on the padded coefficients.
    // The `in_place_nr_2radix_fft` function expects and produces bit-reversed evaluations
    // when used with bit-reversed twiddles.
    let mut p1_evals_bit_rev = p1_coeffs;
    in_place_nr_2radix_fft(&mut p1_evals_bit_rev, twiddles);

    let mut p2_evals_bit_rev = p2_coeffs;
    in_place_nr_2radix_fft(&mut p2_evals_bit_rev, twiddles);

    // 3. Apply bit-reversal permutation to get naturally ordered evaluations.
    // This step converts the bit-reversed output of the FFT into standard order.
    let mut p1_evals = p1_evals_bit_rev;
    in_place_bit_reverse_permute(&mut p1_evals);

    let mut p2_evals = p2_evals_bit_rev;
    in_place_bit_reverse_permute(&mut p2_evals);

    // 4. Perform pointwise multiplication of the evaluations.
    // This is the core step where the polynomial multiplication in the coefficient domain
    // is transformed into simple element-wise multiplication in the evaluation domain.
    let c_evals: Vec<FE> = p1_evals
        .iter()
        .zip(p2_evals.iter()) // Iterate over both evaluation vectors simultaneously.
        .map(|(y1, y2)| y1 * y2) // Multiply corresponding evaluations.
        .collect();

    // 5. Perform Inverse Fast Fourier Transform (IFFT) on the product evaluations.
    // This transforms the product evaluations back into product coefficients (still bit-reversed).
    let mut c_coeffs_bit_rev = c_evals;
    in_place_nr_2radix_fft(&mut c_coeffs_bit_rev, inv_twiddles);

    // 6. Apply bit-reversal permutation again to get naturally ordered, scaled coefficients.
    let mut c_coeffs_scaled = c_coeffs_bit_rev;
    in_place_bit_reverse_permute(&mut c_coeffs_scaled);

    // 7. Scale the coefficients by 1/N.
    // The IFFT process introduces a scaling factor of N (the domain size),
    // so we need to divide each coefficient by N to get the true coefficients.
    let n_inv = FE::from(n as u64)
        .inv()
        .expect("Inverse of N should exist in the field.");
    let c_coeffs: Vec<FE> = c_coeffs_scaled.iter().map(|c| c * n_inv).collect();

    // 8. Construct the resulting polynomial from the computed coefficients.
    Polynomial::new(&c_coeffs)
}

/// Multiplies two polynomials using a naive O(N^2) algorithm.
///
/// This function serves as a reference implementation for correctness verification
/// and to demonstrate the performance difference compared to the FFT-based approach.
///
/// # Arguments
/// * `p1` - The first polynomial.
/// * `p2` - The second polynomial.
///
/// # Returns
/// A new `Polynomial` representing the product `p1 * p2`.
pub fn multiply_polynomials_naive(p1: &Polynomial<FE>, p2: &Polynomial<FE>) -> Polynomial<FE> {
    let deg1 = p1.degree();
    let deg2 = p2.degree();

    // The degree of the product polynomial is deg1 + deg2.
    // The number of coefficients will be deg1 + deg2 + 1.
    let mut result_coeffs = vec![FE::zero(); deg1 + deg2 + 1];

    // Perform the standard polynomial multiplication by iterating through
    // each coefficient of p1 and multiplying it by each coefficient of p2.
    // The product of x^i and x^j contributes to the x^(i+j) term.
    for i in 0..=deg1 {
        for j in 0..=deg2 {
            result_coeffs[i + j] += p1.coefficients[i] * p2.coefficients[j];
        }
    }

    Polynomial::new(&result_coeffs)
}

pub mod strategies {
    use proptest::collection::vec;
    use proptest::prelude::{any, Strategy};

    use super::*;

    pub fn next_power_of_2(n: usize) -> usize {
        let mut k = 1;
        while k < n {
            k <<= 1;
        }
        k
    }

    /// Generates a polynomial with coefficients as `FE` elements,
    /// with a degree up to `max_degree`.
    pub fn arb_polynomial(max_degree: usize) -> impl Strategy<Value = Polynomial<FE>> {
        // Generate a vector of coefficients. The range `1..=max_degree`
        // ensures that the polynomial has at least one term (a constant).
        vec(any::<u64>().prop_map(FE::from), 1..=max_degree)
            .prop_map(|coeffs| Polynomial::new(&coeffs))
    }
}

#[cfg(test)]
mod tests {
    use lambdaworks_math::fft::cpu::roots_of_unity::get_twiddles;
    use lambdaworks_math::field::fields::fft_friendly::babybear_u32::Babybear31PrimeField;
    use lambdaworks_math::field::traits::RootsConfig;
    use proptest::prop_assert_eq;
    use proptest::test_runner::{Config, TestRunner};

    use crate::strategies::{self, next_power_of_2};
    use crate::{multiply_polynomials_fft, multiply_polynomials_naive};

    /// This test verifies that the FFT multiplication produces the same result
    /// as the naive multiplication for a range of randomly generated polynomials.
    #[test]
    fn proptest_fft_vs_naive_multiplication() {
        let mut runner = TestRunner::new(Config::default());

        let max_degree_for_proptest = 1000;

        let strategy = (
            strategies::arb_polynomial(max_degree_for_proptest),
            strategies::arb_polynomial(max_degree_for_proptest),
        );

        runner
            .run(&strategy, |(p1, p2)| {
                // Calculate expected result using the naive method.
                let expected_poly = multiply_polynomials_naive(&p1, &p2);

                // Determine the FFT domain size and precompute twiddles.
                let min_domain_size = p1.degree() + p2.degree() + 1;
                let n = next_power_of_2(min_domain_size);
                let twiddles = get_twiddles::<Babybear31PrimeField>(
                    n.trailing_zeros() as u64,
                    RootsConfig::BitReverse,
                )
                .unwrap();
                let inv_twiddles = get_twiddles::<Babybear31PrimeField>(
                    n.trailing_zeros() as u64,
                    RootsConfig::BitReverseInversed,
                )
                .unwrap();

                // Calculate actual result using the FFT method.
                let actual_poly = multiply_polynomials_fft(&p1, &p2, n, &twiddles, &inv_twiddles);

                // Assert that the coefficients are equal.
                prop_assert_eq!(
                    actual_poly.coefficients,
                    expected_poly.coefficients,
                    "FFT and Naive multiplication results differ!"
                );
                Ok(())
            })
            .unwrap();
    }
}
</file>

<file path="3_polynomial_commitment_scheme/src/error.rs">
use std::fmt;

#[derive(Debug, PartialEq, Eq)]
pub enum FriError {
    /// Error building a Merkle tree for a specific layer.
    MerkleTreeConstructionError(String),
    /// A Merkle proof verification failed.
    InvalidMerkleProof,
    /// The folding process at a specific layer was inconsistent.
    InconsistentFolding {
        layer: usize,
        expected: String,
        got: String,
    },
}

impl fmt::Display for FriError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            FriError::MerkleTreeConstructionError(msg) => {
                write!(f, "Merkle tree construction failed: {}", msg)
            }
            FriError::InvalidMerkleProof => write!(f, "Invalid Merkle proof"),
            FriError::InconsistentFolding {
                layer,
                expected,
                got,
            } => write!(
                f,
                "Inconsistent folding at layer {}: expected {}, got {}",
                layer, expected, got
            ),
        }
    }
}
</file>

<file path="4_air_constraints_design/src/composition.rs">
// ================================================================================================
// PART 3: COMPOSITION & OUT-OF-DOMAIN SAMPLING
// ================================================================================================
// To reduce the number of polynomials the Verifier needs to check, the Prover combines them
// into a single "composition polynomial" H(x).

use lambdaworks_math::polynomial::Polynomial;

use crate::arithmetization::Arithmetization;
use crate::{F, FE};

/// Holds the composition polynomial and its related data.
pub struct Composition {
    // LDE of the composition polynomial H(x).
    pub composition_poly_lde: Vec<FE>,
    // Coefficient form of H(x), needed for out-of-domain evaluation.
    pub composition_poly: Polynomial<FE>,
}

impl Composition {
    /// Combines constraint polynomials into a single composition polynomial H(x).
    /// H(x) = α₁ * B(x) + α₂ * T(x)
    /// The Verifier provides random challenges (α₁, α₂) to ensure the Prover can't cheat.
    pub fn new(arithmetization: &Arithmetization, alpha1: &FE, alpha2: &FE) -> Self {
        println!("\n-- STEP 3: POLYNOMIAL COMPOSITION -----------------------------");
        let composition_poly_lde = arithmetization
            .boundary_constraint_poly_lde
            .iter()
            .zip(&arithmetization.transition_constraint_poly_lde)
            .map(|(b_eval, t_eval)| b_eval * alpha1 + t_eval * alpha2)
            .collect::<Vec<_>>();

        // Interpolate to get the coefficient form. This is needed for the OOD check.
        let composition_poly =
            Polynomial::interpolate_offset_fft::<F>(&composition_poly_lde, &FE::from(3)).unwrap();

        println!(
            "  [3.1] Combined constraints into composition polynomial H(x) of degree {}.",
            composition_poly.degree()
        );
        println!("        The Prover commits to H(x) (e.g., via a Merkle tree of its LDE).");

        Self {
            composition_poly_lde,
            composition_poly,
        }
    }

    /// Simulates the out-of-domain check (the "DEEP" part of STARKs begins here).
    /// The Verifier asks the Prover to evaluate polynomials at a random point 'z' that is
    /// *not* in the LDE domain. This forces the Prover to have committed to actual low-degree
    /// polynomials, not just arbitrary values.
    pub fn perform_ood_check(
        &self,
        arithmetization: &Arithmetization,
        alpha1: &FE,
        alpha2: &FE,
        z: &FE,
    ) {
        println!("\n-- STEP 4: OUT-OF-DOMAIN SAMPLING (OOD) -----------------------");
        let g = &arithmetization.domain_generator;

        // Prover evaluates the trace polynomial at z and its required shifts (z*g, z*g^2),
        // and the composition polynomial H(z). These evaluations are sent to the verifier.
        let t_z = arithmetization.trace_poly.evaluate(z);
        let t_zg = arithmetization.trace_poly.evaluate(&(z * g));
        let t_zg2 = arithmetization.trace_poly.evaluate(&(z * g.square()));
        let h_z = self.composition_poly.evaluate(z);
        println!("  --> Prover to Verifier: Send evaluations at random point z.");
        println!(
            "      t(z)={}, t(z*g)={}, t(z*g^2)={}, H(z)={}",
            t_z.representative(),
            t_zg.representative(),
            t_zg2.representative(),
            h_z.representative()
        );

        // Verifier uses these evaluations to reconstruct H(z) on its own.
        // It computes the boundary and transition constraints at 'z' using the claimed t(z) values.
        println!("  <-- Verifier: Reconstructs H(z) to check consistency.");
        let boundary_interpolant = Polynomial::interpolate(
            &[arithmetization.domain[0], arithmetization.domain[1]],
            &[FE::one(), FE::one()],
        )
        .unwrap();
        let boundary_zerofier_z = (z - arithmetization.domain[0]) * (z - arithmetization.domain[1]);
        let boundary_eval_z =
            (t_z - boundary_interpolant.evaluate(z)) * boundary_zerofier_z.inv().unwrap();

        let transition_zerofier_z = {
            let numerator = z.pow(arithmetization.trace_length) - FE::one();
            let exemptions_at_z = (z - arithmetization.domain[arithmetization.trace_length - 2])
                * (z - arithmetization.domain[arithmetization.trace_length - 1]);
            numerator * exemptions_at_z.inv().unwrap()
        };
        let transition_eval_z = (t_zg2 - t_zg - t_z) * transition_zerofier_z.inv().unwrap();

        let h_z_reconstructed = boundary_eval_z * alpha1 + transition_eval_z * alpha2;

        println!(
            "      Reconstructed H(z): {}",
            h_z_reconstructed.representative()
        );
        assert_eq!(h_z, h_z_reconstructed, "Out-of-domain check failed!");
        println!("  [4.1] SUCCESS: Verifier's reconstructed H(z) matches Prover's H(z).");
    }
}
</file>

<file path="4_air_constraints_design/src/deep_composition.rs">
// ================================================================================================
// PART 4: DEEP COMPOSITION POLYNOMIAL
// ================================================================================================
// All the evaluation claims from the OOD check are bundled together into a single polynomial,
// the "DEEP" composition polynomial. Proving this single polynomial has a low degree is
// equivalent to proving all the original claims simultaneously.

use lambdaworks_math::polynomial::Polynomial;

use crate::arithmetization::Arithmetization;
use crate::composition::Composition;
use crate::{F, FE};

/// Holds the DEEP composition polynomial.
pub struct DeepComposition {
    deep_poly_lde: Vec<FE>,
}

impl DeepComposition {
    /// Constructs the DEEP composition polynomial's evaluations over the LDE domain.
    /// D(x) = β₀ * (H(x) - H(z))/(x - z) +
    ///        β₁ * (t(x) - t(z))/(x - z) +
    ///        β₂ * (t(x) - t(zg))/(x - zg) +
    ///        β₃ * (t(x) - t(zg^2))/(x - zg^2)
    /// Each term will be a polynomial if and only if the numerator is zero when the
    /// denominator is zero (i.e., the evaluation claims are correct).
    pub fn new(
        arithmetization: &Arithmetization,
        composition: &Composition,
        z: &FE,
        betas: &[FE; 4],
    ) -> Self {
        println!("\n-- STEP 5: DEEP COMPOSITION -----------------------------------");
        println!("The Prover creates the DEEP polynomial D(x) to bundle all OOD claims.");
        let g = &arithmetization.domain_generator;

        // Pre-evaluate polynomials at OOD points (Prover already has these).
        let h_z = composition.composition_poly.evaluate(z);
        let t_z = arithmetization.trace_poly.evaluate(z);
        let t_zg = arithmetization.trace_poly.evaluate(&(z * g));
        let t_zg2 = arithmetization.trace_poly.evaluate(&(z * g.square()));

        // Get evaluations on LDE domain.
        let h_lde = &composition.composition_poly_lde;
        let t_lde = arithmetization
            .trace_poly
            .evaluate_slice(&arithmetization.lde_domain);

        // Compute point-wise evaluations for each term of the DEEP polynomial.
        let h_term_lde = h_lde
            .iter()
            .zip(&arithmetization.lde_domain)
            .map(|(h_xi, xi)| (h_xi - h_z) * (xi - z).inv().unwrap())
            .collect::<Vec<_>>();

        let t_z_term_lde = t_lde
            .iter()
            .zip(&arithmetization.lde_domain)
            .map(|(t_xi, xi)| (t_xi - t_z) * (xi - z).inv().unwrap())
            .collect::<Vec<_>>();

        let t_zg_term_lde = t_lde
            .iter()
            .zip(&arithmetization.lde_domain)
            .map(|(t_xi, xi)| (t_xi - t_zg) * (xi - (z * g)).inv().unwrap())
            .collect::<Vec<_>>();

        let t_zg2_term_lde = t_lde
            .iter()
            .zip(&arithmetization.lde_domain)
            .map(|(t_xi, xi)| (t_xi - t_zg2) * (xi - (z * g.square())).inv().unwrap())
            .collect::<Vec<_>>();

        // Combine the terms with random weights (betas) from the Verifier.
        let deep_poly_lde = h_term_lde
            .iter()
            .zip(&t_z_term_lde)
            .zip(&t_zg_term_lde)
            .zip(&t_zg2_term_lde)
            .map(|(((h_term, t_z_term), t_zg_term), t_zg2_term)| {
                h_term * betas[0]
                    + t_z_term * betas[1]
                    + t_zg_term * betas[2]
                    + t_zg2_term * betas[3]
            })
            .collect::<Vec<_>>();

        // The Prover would now run FRI on the `deep_poly_lde` to prove it has a low degree.
        // We will simulate the final check of that protocol.
        let deep_poly_coeffs =
            Polynomial::interpolate_offset_fft::<F>(&deep_poly_lde, &FE::from(3)).unwrap();
        println!(
            "  [5.1] Constructed DEEP polynomial D(x) of degree {}.",
            deep_poly_coeffs.degree()
        );
        println!(
            "        The Prover commits to D(x) and generates a FRI proof of its low-degreeness."
        );

        Self { deep_poly_lde }
    }

    /// Simulates the final spot-check after the FRI protocol.
    /// The FRI protocol gives the Verifier a random point `x₀` from the LDE domain and the
    /// claimed evaluations of the committed polynomials at that point. The Verifier checks if
    /// these values are consistent.
    pub fn perform_final_spot_check(
        &self,
        arithmetization: &Arithmetization,
        composition: &Composition,
        z: &FE,
        betas: &[FE; 4],
        x0_index: usize, // Index of a point in the LDE domain from a FRI query.
    ) {
        println!("\n-- STEP 6: FINAL CONSISTENCY CHECK --------------------------");
        let x0 = &arithmetization.lde_domain[x0_index];
        println!(
            "The Verifier picks a random point x₀={} from the LDE domain (via FRI).",
            x0.representative()
        );

        // Prover provides evaluations D(x₀), H(x₀), and t(x₀), authenticated by Merkle paths.
        let deep_x0 = self.deep_poly_lde[x0_index];
        let h_x0 = composition.composition_poly_lde[x0_index];
        let t_x0 = arithmetization.trace_poly.evaluate(x0);
        println!("  --> Prover to Verifier: Openings at x₀.");
        println!(
            "      D(x₀)={}, H(x₀)={}, t(x₀)={}",
            deep_x0.representative(),
            h_x0.representative(),
            t_x0.representative()
        );

        // Verifier reconstructs D(x₀) using the provided H(x₀), t(x₀) and the OOD
        // values it received earlier.
        println!("  <-- Verifier: Reconstructs D(x₀) to check final consistency.");
        let g = &arithmetization.domain_generator;

        // These OOD values are already known and trusted by the verifier from Step 4.
        let h_z = composition.composition_poly.evaluate(z);
        let t_z = arithmetization.trace_poly.evaluate(z);
        let t_zg = arithmetization.trace_poly.evaluate(&(z * g));
        let t_zg2 = arithmetization.trace_poly.evaluate(&(z * g.square()));

        let h_term_recon = (h_x0 - h_z) * (x0 - z).inv().unwrap();
        let t_z_term_recon = (t_x0 - t_z) * (x0 - z).inv().unwrap();
        let t_zg_term_recon = (t_x0 - t_zg) * (x0 - (z * g)).inv().unwrap();
        let t_zg2_term_recon = (t_x0 - t_zg2) * (x0 - (z * g.square())).inv().unwrap();

        let deep_x0_reconstructed = h_term_recon * betas[0]
            + t_z_term_recon * betas[1]
            + t_zg_term_recon * betas[2]
            + t_zg2_term_recon * betas[3];

        println!(
            "      Reconstructed D(x₀): {}",
            deep_x0_reconstructed.representative()
        );
        assert_eq!(deep_x0, deep_x0_reconstructed, "Final spot check failed!");
        println!("  [6.1] SUCCESS: All polynomial commitments are consistent.");
    }
}
</file>

<file path="4_air_constraints_design/src/trace.rs">
// ================================================================================================
// PART 1: COMPUTATION & EXECUTION TRACE
// ================================================================================================
// A STARK proof starts with a computation. We represent this computation as an "execution trace".
// For this example, our computation is the Fibonacci sequence.

use crate::FE;

/// Generates a Fibonacci sequence trace of a given length.
pub fn generate_fibonacci_trace(trace_length: usize) -> Vec<FE> {
    let mut trace = vec![FE::zero(); trace_length];
    // Set the initial values, which act as our boundary conditions.
    trace[0] = FE::one();
    trace[1] = FE::one();

    for i in 2..trace_length {
        trace[i] = trace[i - 1] + trace[i - 2];
    }
    trace
}
</file>

<file path="rustfmt.toml">
# See: https://rust-lang.github.io/rustfmt
normalize_comments = true
use_field_init_shorthand = true

# Unstable
comment_width = 100
condense_wildcard_suffixes = true
format_code_in_doc_comments = true
group_imports = "StdExternalCrate"
imports_granularity = "Module"
unstable_features = true
wrap_comments = true
</file>

<file path="typos.toml">
# typos Configuration for the Zero-Knowledge Proofs Deep Dive Repository
#
# This file configures the 'typos' spell-checker to understand the
# domain-specific terminology used in this project.
# See https://github.com/crate-ci/typos for more configuration options.

[files]
# By default, typos respects .gitignore, so 'target/' and other ignored paths
# are already excluded. No further file-level configuration is necessary.

# Add words that are considered correct for this project to the dictionary.
[default.extend-words]
# --- ZKP & Cryptography Acronyms ---
AIR = "AIR"                 # Algebraic Intermediate Representation
FRI = "FRI"                 # Fast Reed-Solomon Interactive Oracle Proof of Proximity
Groth = "Groth"             # Jens Groth (e.g., Groth16)
IPA = "IPA"                 # Inner Product Argument
IOP = "IOP"                 # Interactive Oracle Proof
KZG = "KZG"                 # Kate-Zaverucha-Goldberg
PCS = "PCS"                 # Polynomial Commitment Scheme
PCSs = "PCSs"               # Plural of PCS
PIOP = "PIOP"               # Polynomial Interactive Oracle Proof
PIOPs = "PIOPs"             # Plural of PIOP
R1CS = "R1CS"               # Rank-1 Constraint System
SNARKs = "SNARKs"           # Succinct Non-interactive ARgument of Knowledge
STARK = "STARK"             # Scalable Transparent ARgument of Knowledge
STARKs = "STARKs"           # Plural of STARK
ZKP = "ZKP"                 # Zero-Knowledge Proof
ZKPs = "ZKPs"               # Plural of ZKP

# --- ZKP System & Protocol Names ---
CircleSTARK = "CircleSTARK"
Halo = "Halo"               # As in Halo2
HyperPlonk = "HyperPlonk"
Plonk = "Plonk"
Plonky = "Plonky"           # As in Plonky2, Plonky3
UltraPLONK = "UltraPLONK"

# --- Mathematical & Technical Terms ---
Arithmetization = "Arithmetization" # A core concept in ZKPs
codelets = "codelets"       # Term from high-performance computing (FFTW)
coeffs = "coeffs"           # Common abbreviation for 'coefficients'
evals = "evals"             # Common abbreviation for 'evaluations'
FFT = "FFT"                 # Fast Fourier Transform
FFTW = "FFTW"               # "Fastest Fourier Transform in the West" library
GF = "GF"                   # Galois Field
multivariate = "multivariate"
prover = "prover"
radix = "radix"             # As in radix-2 FFT
twiddle = "twiddle"         # As in "twiddle factors"
twiddles = "twiddles"       # Plural of twiddle
univariate = "univariate"
verifier = "verifier"

# --- Project Dependencies, Tools & Author Handles ---
lambdaclass = "lambdaclass" # Organization name
lambdaworks = "lambdaworks" # Rust dependency from lambdaclass
Okm165 = "Okm165"           # GitHub handle Okm165
bartolomeodiaz = "bartolomeodiaz"   # Twitter handle
proptest = "proptest"       # Rust testing dependency
rustfmt = "rustfmt"         # Rust code formatter

# typos is generally aware of programming language keywords,
# so it's not necessary to add Rust keywords like 'struct', 'fn', 'impl', etc.
</file>

<file path="1_mathematical_toolkit/src/tests.rs">
use proptest::prelude::*;

use crate::PrimeField;

// Define the field GF(7)
pub const PRIME: u64 = (1 << 31) - 1;
pub const FIELD: PrimeField = PrimeField { p: PRIME };

// Closure property for addition and multiplication:
// The result of a + b and a * b must remain in the field (i.e., less than p)
proptest! {
    #[test]
    fn closure_add_mul(a in 0..FIELD.p, b in 0..FIELD.p) {
        let sum = FIELD.add(a, b);
        let product = FIELD.mul(a, b);
        prop_assert!(sum < FIELD.p);
        prop_assert!(product < FIELD.p);
    }
}

// Associativity property:
// (a + b) + c == a + (b + c)
// (a * b) * c == a * (b * c)
proptest! {
    #[test]
    fn associativity_add_mul(a in 0..FIELD.p, b in 0..FIELD.p, c in 0..FIELD.p) {
        prop_assert_eq!(FIELD.add(FIELD.add(a, b), c), FIELD.add(a, FIELD.add(b, c)));
        prop_assert_eq!(FIELD.mul(FIELD.mul(a, b), c), FIELD.mul(a, FIELD.mul(b, c)));
    }
}

// Commutativity property:
// a + b == b + a
// a * b == b * a
proptest! {
    #[test]
    fn commutativity_add_mul(a in 0..FIELD.p, b in 0..FIELD.p) {
        prop_assert_eq!(FIELD.add(a, b), FIELD.add(b, a));
        prop_assert_eq!(FIELD.mul(a, b), FIELD.mul(b, a));
    }
}

// Identity elements:
// a + 0 == a (additive identity)
// a * 1 == a (multiplicative identity)
proptest! {
    #[test]
    fn identity_elements(a in 0..FIELD.p) {
        prop_assert_eq!(FIELD.add(a, 0), a);
        prop_assert_eq!(FIELD.mul(a, 1), a);
    }
}

// Additive inverse:
// a + (-a) == 0
proptest! {
    #[test]
    fn additive_inverse(a in 0..FIELD.p) {
        prop_assert_eq!(FIELD.add(a, FIELD.neg(a)), 0);
    }
}

// Multiplicative inverse:
// a * a^-1 == 1, for all a != 0
proptest! {
    #[test]
    fn multiplicative_inverse(a in 1..FIELD.p) {
        let inv = FIELD.inv(a);
        prop_assert_eq!(FIELD.mul(a, inv), 1);
    }
}

// Distributivity:
// a * (b + c) == a * b + a * c
proptest! {
    #[test]
    fn distributivity(a in 0..FIELD.p, b in 0..FIELD.p, c in 0..FIELD.p) {
        let left = FIELD.mul(a, FIELD.add(b, c));
        let right = FIELD.add(FIELD.mul(a, b), FIELD.mul(a, c));
        prop_assert_eq!(left, right);
    }
}

proptest! {
    #[test]
    #[should_panic]
    fn associativity_add_float(a in 0_f64..10_f64, b in 0_f64..10_f64, c in 0_f64..10_f64) {
        prop_assert_eq!((a + b) + c, a + (b + c));
    }
}

proptest! {
    #[test]
    #[should_panic]
    fn associativity_mul_float(a in 0_f64..10_f64, b in 0_f64..10_f64, c in 0_f64..10_f64) {
        prop_assert_eq!((a * b) * c, a * (b * c));
    }
}
</file>

<file path="1_mathematical_toolkit/Cargo.toml">
[package]
name = "mathematical_toolkit"
edition.workspace = true
version.workspace = true
readme = "README.md"

[dependencies]
proptest = "1.7.0"
</file>

<file path="2_fast_polynomial_arithmetic/benches/lines.svg">
<svg width="960" height="540" viewBox="0 0 960 540" xmlns="http://www.w3.org/2000/svg">
<text x="480" y="5" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="16.129032258064516" opacity="1" fill="#000000">
Polynomial Multiplication Comparison: Comparison
</text>
<text x="26" y="263" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000" transform="rotate(270, 26, 263)">
Average time (ms)
</text>
<text x="510" y="514" dy="-0.5ex" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
Input
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="85,52 85,473 "/>
<text x="76" y="421" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
0.5
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,421 85,421 "/>
<text x="76" y="369" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
1.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,369 85,369 "/>
<text x="76" y="316" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
1.5
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,316 85,316 "/>
<text x="76" y="264" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
2.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,264 85,264 "/>
<text x="76" y="211" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
2.5
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,211 85,211 "/>
<text x="76" y="159" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
3.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,159 85,159 "/>
<text x="76" y="106" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
3.5
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,106 85,106 "/>
<text x="76" y="54" dy="0.5ex" text-anchor="end" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
4.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="80,54 85,54 "/>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="86,474 933,474 "/>
<text x="224" y="484" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
500.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="224,474 224,479 "/>
<text x="366" y="484" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
1000.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="366,474 366,479 "/>
<text x="508" y="484" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
1500.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="508,474 508,479 "/>
<text x="649" y="484" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
2000.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="649,474 649,479 "/>
<text x="791" y="484" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
2500.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="791,474 791,479 "/>
<text x="933" y="484" dy="0.76em" text-anchor="middle" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
3000.0
</text>
<polyline fill="none" opacity="1" stroke="#000000" stroke-width="1" points="933,474 933,479 "/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="88" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="88" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="88" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="90" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="90" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="90" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="92" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="92" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="92" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="94" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="94" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="94" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="96" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="96" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="96" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="97" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="97" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="97" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="98" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="99" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="99" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="100" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="100" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="101" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="101" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="102" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="102" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="103" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="104" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="104" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="105" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="105" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="106" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="106" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="107" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="108" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="108" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="109" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="109" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="110" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="110" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="111" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="114" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="117" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="119" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="122" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="125" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="128" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="131" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="134" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="136" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="139" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="142" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="145" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="148" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="151" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="153" cy="473" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="156" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="159" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="162" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="165" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="168" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="175" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="182" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="189" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="196" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="203" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="210" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="217" cy="472" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="224" cy="471" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="238" cy="471" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="253" cy="471" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="267" cy="471" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="281" cy="471" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="295" cy="470" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="309" cy="470" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="323" cy="470" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="338" cy="470" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="352" cy="470" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="366" cy="469" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="394" cy="469" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="423" cy="469" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="451" cy="468" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="479" cy="468" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="508" cy="467" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="536" cy="467" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="564" cy="466" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="593" cy="466" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="621" cy="466" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="649" cy="465" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="678" cy="465" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="706" cy="464" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="734" cy="464" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="763" cy="464" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="791" cy="463" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="819" cy="462" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="848" cy="462" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="876" cy="461" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="904" cy="461" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<circle cx="933" cy="461" r="3" opacity="1" fill="#B22222" stroke="none" stroke-width="1"/>
<polyline fill="none" opacity="1" stroke="#B22222" stroke-width="1" points="86,473 86,473 86,473 86,473 87,473 87,473 87,473 87,473 88,473 88,473 88,473 89,473 89,473 89,473 89,473 90,473 90,473 90,473 91,473 91,473 91,473 91,473 92,473 92,473 92,473 93,473 93,473 93,473 93,473 94,473 94,473 94,473 95,473 95,473 95,473 95,473 96,473 96,473 96,473 97,473 97,473 97,473 98,473 99,473 99,473 100,473 100,473 101,473 101,473 102,473 102,473 103,473 104,473 104,473 105,473 105,473 106,473 106,473 107,473 108,473 108,473 109,473 109,473 110,473 110,473 111,473 114,473 117,473 119,473 122,473 125,473 128,473 131,473 134,473 136,473 139,473 142,473 145,473 148,473 151,473 153,473 156,472 159,472 162,472 165,472 168,472 175,472 182,472 189,472 196,472 203,472 210,472 217,472 224,471 238,471 253,471 267,471 281,471 295,470 309,470 323,470 338,470 352,470 366,469 394,469 423,469 451,468 479,468 508,467 536,467 564,466 593,466 621,466 649,465 678,465 706,464 734,464 763,464 791,463 819,462 848,462 876,461 904,461 933,461 "/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="86" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="87" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="88" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="88" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="88" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="89" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="90" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="90" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="90" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="91" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="92" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="92" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="92" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="93" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="94" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="94" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="94" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="95" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="96" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="96" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="96" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="97" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="97" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="97" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="98" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="99" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="99" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="100" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="100" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="101" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="101" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="102" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="102" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="103" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="104" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="104" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="105" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="105" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="106" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="106" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="107" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="108" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="108" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="109" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="109" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="110" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="110" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="111" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="114" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="117" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="119" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="122" cy="473" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="125" cy="472" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="128" cy="472" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="131" cy="472" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="134" cy="472" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="136" cy="472" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="139" cy="472" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="142" cy="471" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="145" cy="471" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="148" cy="471" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="151" cy="471" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="153" cy="471" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="156" cy="470" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="159" cy="470" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="162" cy="469" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="165" cy="469" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="168" cy="469" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="175" cy="468" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="182" cy="468" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="189" cy="467" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="196" cy="466" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="203" cy="465" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="210" cy="464" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="217" cy="463" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="224" cy="462" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="238" cy="456" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="253" cy="456" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="267" cy="453" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="281" cy="451" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="295" cy="446" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="309" cy="444" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="323" cy="437" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="338" cy="435" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="352" cy="432" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="366" cy="428" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="394" cy="416" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="423" cy="405" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="451" cy="394" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="479" cy="380" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="508" cy="372" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="536" cy="354" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="564" cy="340" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="593" cy="316" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="621" cy="304" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="649" cy="288" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="678" cy="268" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="706" cy="248" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="734" cy="221" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="763" cy="208" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="791" cy="182" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="819" cy="157" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="848" cy="137" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="876" cy="104" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="904" cy="88" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<circle cx="933" cy="52" r="3" opacity="1" fill="#2E8B57" stroke="none" stroke-width="1"/>
<polyline fill="none" opacity="1" stroke="#2E8B57" stroke-width="1" points="86,473 86,473 86,473 86,473 87,473 87,473 87,473 87,473 88,473 88,473 88,473 89,473 89,473 89,473 89,473 90,473 90,473 90,473 91,473 91,473 91,473 91,473 92,473 92,473 92,473 93,473 93,473 93,473 93,473 94,473 94,473 94,473 95,473 95,473 95,473 95,473 96,473 96,473 96,473 97,473 97,473 97,473 98,473 99,473 99,473 100,473 100,473 101,473 101,473 102,473 102,473 103,473 104,473 104,473 105,473 105,473 106,473 106,473 107,473 108,473 108,473 109,473 109,473 110,473 110,473 111,473 114,473 117,473 119,473 122,473 125,472 128,472 131,472 134,472 136,472 139,472 142,471 145,471 148,471 151,471 153,471 156,470 159,470 162,469 165,469 168,469 175,468 182,468 189,467 196,466 203,465 210,464 217,463 224,462 238,456 253,456 267,453 281,451 295,446 309,444 323,437 338,435 352,432 366,428 394,416 423,405 451,394 479,380 508,372 536,354 564,340 593,316 621,304 649,288 678,268 706,248 734,221 763,208 791,182 819,157 848,137 876,104 904,88 933,52 "/>
<text x="131" y="67" dy="0.76em" text-anchor="start" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
FFT
</text>
<text x="131" y="82" dy="0.76em" text-anchor="start" font-family="sans-serif" font-size="9.67741935483871" opacity="1" fill="#000000">
Naive
</text>
<rect x="101" y="67" width="20" height="10" opacity="1" fill="#B22222" stroke="none"/>
<rect x="101" y="82" width="20" height="10" opacity="1" fill="#2E8B57" stroke="none"/>
</svg>
</file>

<file path="2_fast_polynomial_arithmetic/benches/polynomial_multiplication.rs">
//! This module contains benchmarks for polynomial multiplication
//! using both FFT-based and naive algorithms.
//!
//! Benchmarks are implemented using the `criterion` crate.

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};
use fast_polynomial_arithmetic::{
    multiply_polynomials_fft, multiply_polynomials_naive, strategies,
};
use lambdaworks_math::fft::cpu::roots_of_unity::get_twiddles;
use lambdaworks_math::field::fields::fft_friendly::babybear_u32::Babybear31PrimeField;
use lambdaworks_math::field::traits::RootsConfig;
use proptest::prelude::*;
use proptest::strategy::ValueTree;
use proptest::test_runner::TestRunner;

// --- Combined Benchmarks ---

fn polynomial_multiplication_benchmark(c: &mut Criterion) {
    let mut group = c.benchmark_group("Polynomial Multiplication Comparison");

    let mut runner = TestRunner::default();

    let degrees = vec![
        10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,
        33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 54, 56, 58, 60,
        62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 110, 120,
        130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
        325, 350, 375, 400, 425, 450, 475, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000,
        1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500,
        2600, 2700, 2800, 2900, 3000,
    ];

    for &deg in degrees.iter() {
        // Benchmark FFT multiplication
        group.bench_with_input(BenchmarkId::new("FFT", deg), &deg, |b, &deg_val| {
            let strategy = (
                strategies::arb_polynomial(deg_val),
                strategies::arb_polynomial(deg_val),
            );
            b.iter_batched(
                || {
                    let (p1, p2) = strategy.new_tree(&mut runner).unwrap().current();
                    let min_domain_size = p1.degree() + p2.degree() + 1;
                    let n = strategies::next_power_of_2(min_domain_size);
                    let twiddles = get_twiddles::<Babybear31PrimeField>(
                        n.trailing_zeros() as u64,
                        RootsConfig::BitReverse,
                    )
                    .unwrap();
                    let inv_twiddles = get_twiddles::<Babybear31PrimeField>(
                        n.trailing_zeros() as u64,
                        RootsConfig::BitReverseInversed,
                    )
                    .unwrap();
                    (p1, p2, n, twiddles, inv_twiddles)
                },
                |(p1, p2, n, twiddles, inv_twiddles)| {
                    black_box(multiply_polynomials_fft(
                        &p1,
                        &p2,
                        n,
                        &twiddles,
                        &inv_twiddles,
                    ))
                },
                criterion::BatchSize::LargeInput,
            );
        });

        // Benchmark Naive multiplication
        group.bench_with_input(BenchmarkId::new("Naive", deg), &deg, |b, &deg_val| {
            let strategy = (
                strategies::arb_polynomial(deg_val),
                strategies::arb_polynomial(deg_val),
            );
            b.iter_batched(
                || {
                    let (p1, p2) = strategy.new_tree(&mut runner).unwrap().current();
                    (p1, p2)
                },
                |(p1, p2)| black_box(multiply_polynomials_naive(&p1, &p2)),
                criterion::BatchSize::LargeInput,
            );
        });
    }

    group.finish();
}

criterion_group! {
    name = benches;
    config = Criterion::default();
    targets = polynomial_multiplication_benchmark
}
criterion_main!(benches);
</file>

<file path="2_fast_polynomial_arithmetic/src/main.rs">
//! This `main` module demonstrates the usage of the FFT-based polynomial multiplication
//! with a concrete example using Babybear31PrimeField.
//! It sets up two polynomials, calculates their product using the `multiply_polynomials_fft`
//! function, and then verifies the result against a known expected polynomial.

// Import the FFT multiplication function from your library.
// Make sure this path is correct based on your crate structure.
use fast_polynomial_arithmetic::multiply_polynomials_fft;
use lambdaworks_math::fft::cpu::roots_of_unity::get_twiddles;
use lambdaworks_math::field::element::FieldElement;
use lambdaworks_math::field::fields::fft_friendly::babybear_u32::Babybear31PrimeField;
use lambdaworks_math::field::traits::RootsConfig;
use lambdaworks_math::polynomial::Polynomial;

// Type aliases for convenience, specifying the field.
type F = Babybear31PrimeField;
type FE = FieldElement<F>;

fn main() {
    // --- SETUP: DEFINE POLYNOMIALS AND DOMAIN ---

    // Define the first polynomial P1(x) = 3x^2 + 2x + 1
    let p1 = Polynomial::new(&[FE::from(1), FE::from(2), FE::from(3)]);
    // Define the second polynomial P2(x) = 5x + 2
    let p2 = Polynomial::new(&[FE::from(2), FE::from(5)]);

    // Calculate the minimum required domain size for FFT.
    // The degree of P1 is 2, and P2 is 1. The product C(x) will have degree 2 + 1 = 3.
    // We need a domain size N such that N >= degree(C) + 1. So, N >= 3 + 1 = 4.
    // The smallest power of 2 that satisfies this is N = 4.
    let n: usize = 4;

    println!("Multiplying polynomials using low-level FFT API:");
    println!("P1(x) = {}", p1.print_as_sage_poly(None));
    println!("P2(x) = {}", p2.print_as_sage_poly(None));
    println!("--------------------------------------\n");

    // Generate bit-reversed twiddle factors (roots of unity) for the forward FFT.
    // The `n.trailing_zeros()` gives log2(N), which is the 'k' for 2^k = N.
    let twiddles = get_twiddles::<F>(n.trailing_zeros() as u64, RootsConfig::BitReverse).unwrap();

    // Generate bit-reversed inverse twiddle factors for the Inverse FFT (IFFT).
    let inv_twiddles =
        get_twiddles::<F>(n.trailing_zeros() as u64, RootsConfig::BitReverseInversed).unwrap();

    // Perform the polynomial multiplication using the FFT algorithm.
    let c_poly = multiply_polynomials_fft(&p1, &p2, n, &twiddles, &inv_twiddles);

    // --- VERIFICATION ---
    println!("--- Verification ---");
    // Expected result: (3x^2 + 2x + 1) * (5x + 2) = 15x^3 + 16x^2 + 9x + 2
    // Coefficients are stored in ascending order of power: [constant, x^1, x^2, x^3]
    let expected_poly = Polynomial::new(&[FE::from(2), FE::from(9), FE::from(16), FE::from(15)]);

    println!(
        "Expected result: {}",
        expected_poly.print_as_sage_poly(None)
    );
    println!("Actual result:   {}", c_poly.print_as_sage_poly(None));

    // Assert that the coefficients of the computed polynomial match the expected coefficients.
    assert_eq!(
        c_poly.coefficients, expected_poly.coefficients,
        "The computed polynomial does not match the expected one."
    );
    println!("\nSuccess! The low-level FFT process produced the correct polynomial.");
}
</file>

<file path="2_fast_polynomial_arithmetic/Cargo.toml">
[package]
name = "fast_polynomial_arithmetic"
edition.workspace = true
version.workspace = true
readme = "README.md"

[dependencies]
lambdaworks-math.workspace = true
proptest.workspace = true

[dev-dependencies]
criterion.workspace = true

[profile.bench]
debug = false # Set to false for accurate performance measurements
lto = true    # Link Time Optimization for better benchmark performance

[[bench]]
name = "polynomial_multiplication"
harness = false # Important: This tells Cargo you're providing your own main function in the bench file
</file>

<file path="3_polynomial_commitment_scheme/src/prover.rs">
use lambdaworks_crypto::fiat_shamir::default_transcript::DefaultTranscript;
use lambdaworks_crypto::fiat_shamir::is_transcript::IsTranscript;
use lambdaworks_crypto::merkle_tree::merkle::MerkleTree;
use lambdaworks_math::polynomial::Polynomial;
use lambdaworks_math::traits::AsBytes;

use crate::error::FriError;
use crate::types::{FriLayer, FriParameters, FriProof, QueryDecommitment};
use crate::{FriBackend, F, FE, PROTOCOL_ID};

/// The Prover entity for the FRI protocol.
pub struct Prover {
    poly: Polynomial<FE>,
    params: FriParameters,
    transcript: DefaultTranscript<F>,
}

impl Prover {
    /// Creates a new Prover.
    pub fn new(poly: Polynomial<FE>, params: FriParameters) -> Self {
        Self {
            poly,
            params,
            transcript: DefaultTranscript::new(PROTOCOL_ID),
        }
    }

    /// Executes the entire proving process.
    pub fn prove(&mut self) -> Result<FriProof, FriError> {
        println!("--- Prover: Starting proof generation ---");

        // 1. Commit Phase: Evaluate the polynomial and commit to the evaluations.
        let initial_layer = self.commit_phase()?;
        // 2. Fold Phase: Recursively fold the polynomial until it's a constant.
        let (layers, last_value) = self.fold_phase(initial_layer)?;
        // 3. Query Phase: Generate decommitments for random queries.
        let query_decommitments = self.query_phase(&layers);

        println!("--- Prover: Proof generation complete ---\n");
        Ok(FriProof {
            layer_commitments: layers.iter().map(|l| l.merkle_tree.root).collect(),
            last_layer_value: last_value,
            query_decommitments,
        })
    }

    /// Phase 1: Commit to the initial polynomial evaluations on the LDE domain.
    fn commit_phase(&mut self) -> Result<FriLayer, FriError> {
        println!("[Prover] Phase 1: COMMIT");
        // Evaluate the polynomial on the large domain (LDE).
        let evaluations = self.poly.evaluate_slice(&self.params.domain);
        // Build a Merkle tree from the evaluations to commit to them.
        let merkle_tree = MerkleTree::<FriBackend>::build(&evaluations).ok_or_else(|| {
            FriError::MerkleTreeConstructionError("Failed to build initial Merkle tree".to_string())
        })?;

        // Add the Merkle root to the transcript to make it part of the public record.
        self.transcript.append_bytes(&merkle_tree.root);
        println!(
            "  > Layer 0 committed with root: 0x{}",
            hex::encode(merkle_tree.root)
        );

        Ok(FriLayer {
            evaluations,
            merkle_tree,
            domain: self.params.domain.to_owned(),
        })
    }

    /// Phase 2: Interactively fold the polynomial evaluations until a constant is reached.
    fn fold_phase(&mut self, initial_layer: FriLayer) -> Result<(Vec<FriLayer>, FE), FriError> {
        println!("[Prover] Phase 2: FOLD");
        let mut layers = vec![initial_layer];

        // Continue folding until the polynomial becomes a constant (evaluations list has 1 element)
        while layers.last().unwrap().evaluations.len() > 1 {
            let i = layers.len() - 1;
            // Get a random challenge `beta` from the transcript.
            let beta: FE = self.transcript.sample_field_element();
            println!(
                "  > Round {}: Sampled challenge beta = {}",
                i,
                beta.representative()
            );

            let previous_layer = layers.last().unwrap();
            // Fold the evaluations and domain for the next layer.
            let (next_evaluations, next_domain) =
                Self::fold_evaluations(&previous_layer.evaluations, &previous_layer.domain, &beta);

            // Commit to the new evaluations.
            let next_merkle_tree =
                MerkleTree::<FriBackend>::build(&next_evaluations).ok_or_else(|| {
                    FriError::MerkleTreeConstructionError(format!(
                        "Failed to build Merkle tree for layer {}",
                        i + 1
                    ))
                })?;

            // Add the new Merkle root to the transcript.
            self.transcript.append_bytes(&next_merkle_tree.root);
            println!(
                "    - Layer {} committed with root: 0x{}",
                i + 1,
                hex::encode(next_merkle_tree.root)
            );

            layers.push(FriLayer {
                evaluations: next_evaluations,
                merkle_tree: next_merkle_tree,
                domain: next_domain,
            });
        }

        // The final layer contains a single evaluation, which is the constant value.
        let last_value = layers.last().unwrap().evaluations[0].clone();
        self.transcript.append_bytes(&last_value.as_bytes());
        println!(
            "  > Folding complete. Final value: {}",
            last_value.representative()
        );

        Ok((layers, last_value))
    }

    /// Phase 3: Generate decommitments for random queries issued by the verifier.
    fn query_phase(&mut self, layers: &[FriLayer]) -> Vec<QueryDecommitment> {
        println!("[Prover] Phase 3: QUERY");
        // Sample random indices from the transcript for the queries.
        let query_indices: Vec<usize> = (0..self.params.num_queries)
            .map(|_| self.sample_index(self.params.domain.len()))
            .collect();

        println!(
            "  > Generating decommitments for queries at indices: {:?}",
            query_indices
        );

        query_indices
            .into_iter()
            .map(|mut query_idx| {
                let mut decommitment = QueryDecommitment {
                    layer_evaluations: Vec::new(),
                    layer_auth_paths: Vec::new(),
                    layer_evaluations_sym: Vec::new(),
                    layer_auth_paths_sym: Vec::new(),
                };

                // For each layer, provide the evaluation and its Merkle proof.
                for layer in layers {
                    let domain_size = layer.domain.len();
                    // The symmetric index corresponds to f(-x).
                    let sym_idx = (query_idx + domain_size / 2) % domain_size;

                    // Provide evaluation and auth path for f(x).
                    decommitment
                        .layer_evaluations
                        .push(layer.evaluations[query_idx].clone());
                    decommitment.layer_auth_paths.push(
                        layer
                            .merkle_tree
                            .get_proof_by_pos(query_idx)
                            .unwrap()
                            .merkle_path,
                    );

                    // Provide evaluation and auth path for f(-x).
                    decommitment
                        .layer_evaluations_sym
                        .push(layer.evaluations[sym_idx].clone());
                    decommitment.layer_auth_paths_sym.push(
                        layer
                            .merkle_tree
                            .get_proof_by_pos(sym_idx)
                            .unwrap()
                            .merkle_path,
                    );

                    // The index for the next layer is `query_idx mod (domain_size / 2)`.
                    query_idx %= (domain_size / 2).max(1);
                }
                decommitment
            })
            .collect()
    }

    /// Folds a layer of evaluations based on a challenge `beta`.
    /// This is the heart of the FRI protocol's recursive step.
    ///
    /// It takes a polynomial `f(x)` represented by its evaluations over a domain `D`,
    /// and computes the evaluations of a new, smaller polynomial `f_next(x^2)` over `D^2`.
    ///
    /// The formula is: `f_next(x^2) = (f(x) + f(-x))/2 + beta * (f(x) - f(-x))/(2x)`
    /// where `(f(x) + f(-x))/2` is the even part of `f` and `(f(x) - f(-x))/(2x)` is the odd part.
    fn fold_evaluations(evaluations: &[FE], domain: &[FE], beta: &FE) -> (Vec<FE>, Vec<FE>) {
        let next_domain_size = domain.len() / 2;
        let two_inv = FE::from(2).inv().unwrap();

        let next_evaluations = (0..next_domain_size)
            .map(|i| {
                // Get the evaluation at a point x and its symmetric counterpart -x
                let y = &evaluations[i];
                let y_symmetric = &evaluations[i + next_domain_size]; // Corresponds to -x

                // Get the domain value x and its inverse
                let x = &domain[i];
                let x_inv = x.inv().unwrap();

                // Calculate the even and odd components of the polynomial
                let f_even = (y + y_symmetric) * &two_inv;
                let f_odd = (y - y_symmetric) * &two_inv * &x_inv;

                // Combine them to get the evaluation of the next polynomial
                f_even + beta * f_odd
            })
            .collect();

        // The next domain consists of the squares of the first half of the current domain
        let next_domain = domain
            .iter()
            .take(next_domain_size)
            .map(|x| x.square())
            .collect();

        (next_evaluations, next_domain)
    }

    /// Samples a random index from the transcript.
    fn sample_index(&mut self, max_value: usize) -> usize {
        // Use 8 bytes from the transcript for a u64, then get a value in range.
        let sample_bytes: [u8; 8] = self.transcript.sample()[..8].try_into().unwrap();
        (u64::from_be_bytes(sample_bytes) % max_value as u64) as usize
    }
}
</file>

<file path="3_polynomial_commitment_scheme/src/types.rs">
use lambdaworks_crypto::merkle_tree::merkle::MerkleTree;
use lambdaworks_math::fft::cpu::roots_of_unity::get_powers_of_primitive_root;
use lambdaworks_math::field::traits::RootsConfig;

use crate::{FriBackend, F, FE};

/// Shared parameters for the FRI protocol, agreed upon by the Prover and Verifier.
#[derive(Debug, Clone)]
pub struct FriParameters {
    /// The initial evaluation domain (LDE).
    pub domain: Vec<FE>,
    /// How many queries the Verifier will make to check the proof.
    pub num_queries: usize,
}

impl FriParameters {
    /// Creates a new set of FRI parameters.
    ///
    /// # Arguments
    /// * `claimed_degree`: The claimed degree of the initial polynomial.
    /// * `blowup_factor`: How much larger the evaluation domain is than the number of coefficients.
    ///   A larger factor provides more security.
    /// * `num_queries`: The number of queries to perform. More queries also increase security.
    pub fn new(claimed_degree: usize, blowup_factor: usize, num_queries: usize) -> Self {
        // The Low-Degree Extension (LDE) domain size.
        let domain_size = (claimed_degree + 1) * blowup_factor;
        // The domain is a multiplicative subgroup, so its size must be a power of 2.
        let root_order = domain_size.trailing_zeros() as u64;

        let domain =
            get_powers_of_primitive_root::<F>(root_order, domain_size, RootsConfig::Natural)
                .unwrap();

        Self {
            domain,
            num_queries,
        }
    }
}

/// Represents a single layer in the FRI protocol's commitment-folding process.
#[derive(Clone)]
pub struct FriLayer {
    /// The evaluations of the polynomial for this layer.
    pub evaluations: Vec<FE>,
    /// The Merkle tree committing to the evaluations.
    pub merkle_tree: MerkleTree<FriBackend>,
    /// The domain over which the evaluations were made.
    pub domain: Vec<FE>,
}

/// A decommitment for a single query, providing evaluations and Merkle paths for each layer.
#[derive(Debug, Clone)]
pub struct QueryDecommitment {
    /// The evaluation at the query index `q` for each layer.
    pub layer_evaluations: Vec<FE>,
    /// The Merkle authentication path for `layer_evaluations` at each layer.
    pub layer_auth_paths: Vec<Vec<[u8; 32]>>,
    /// The evaluation at the symmetric index `-q` for each layer.
    pub layer_evaluations_sym: Vec<FE>,
    /// The Merkle authentication path for `layer_evaluations_sym` at each layer.
    pub layer_auth_paths_sym: Vec<Vec<[u8; 32]>>,
}

/// The complete FRI proof sent from the Prover to the Verifier.
#[derive(Debug, Clone)]
pub struct FriProof {
    /// The Merkle root of each FRI layer.
    pub layer_commitments: Vec<[u8; 32]>,
    /// The value of the final, constant polynomial.
    pub last_layer_value: FE,
    /// The decommitments for each query.
    pub query_decommitments: Vec<QueryDecommitment>,
}
</file>

<file path="3_polynomial_commitment_scheme/src/verifier.rs">
use lambdaworks_crypto::fiat_shamir::default_transcript::DefaultTranscript;
use lambdaworks_crypto::fiat_shamir::is_transcript::IsTranscript;
use lambdaworks_crypto::merkle_tree::proof::Proof;
use lambdaworks_math::field::traits::IsFFTField;
use lambdaworks_math::traits::AsBytes;

use crate::error::FriError;
use crate::types::{FriParameters, FriProof, QueryDecommitment};
use crate::{FriBackend, F, FE, PROTOCOL_ID};

/// The Verifier entity for the FRI protocol.
pub struct Verifier {
    params: FriParameters,
    transcript: DefaultTranscript<F>,
}

impl Verifier {
    /// Creates a new Verifier.
    pub fn new(params: FriParameters) -> Self {
        Self {
            params,
            transcript: DefaultTranscript::new(PROTOCOL_ID),
        }
    }

    /// Verifies the FRI proof.
    pub fn verify(&mut self, proof: &FriProof) -> Result<(), FriError> {
        println!("--- Verifier: Starting verification ---");

        // Reconstruct the challenges (`betas`) and query indices by replaying the transcript.
        let (betas, query_indices) = self.reconstruct_challenges(proof);

        let root_order = self.params.domain.len().trailing_zeros();
        let generator = F::get_primitive_root_of_unity(root_order as u64).unwrap();

        // Verify each query independently.
        for (query_num, &query_idx) in query_indices.iter().enumerate() {
            println!(
                "\n[Verifier] Verifying query #{} (for original index {})",
                query_num + 1,
                query_idx
            );
            self.verify_query(
                proof,
                query_idx,
                &betas,
                &generator,
                &proof.query_decommitments[query_num],
            )?;
        }

        Ok(())
    }

    /// Reconstructs all challenges by replaying the Prover's commitments from the proof.
    /// This ensures the Verifier uses the exact same random values as the Prover.
    fn reconstruct_challenges(&mut self, proof: &FriProof) -> (Vec<FE>, Vec<usize>) {
        // Feed the commitments into the transcript in the same order as the Prover.
        self.transcript.append_bytes(&proof.layer_commitments[0]);
        let betas: Vec<FE> = proof
            .layer_commitments
            .iter()
            .skip(1)
            .map(|commitment| {
                // Sample the field element *before* appending the next commitment.
                let beta = self.transcript.sample_field_element();
                self.transcript.append_bytes(commitment);
                beta
            })
            .collect();

        // Feed the last layer's value.
        self.transcript
            .append_bytes(&proof.last_layer_value.as_bytes());

        // Now, sample the query indices. They will be the same as the Prover's.
        let query_indices = (0..proof.query_decommitments.len())
            .map(|_| self.sample_index(self.params.domain.len()))
            .collect();

        println!("[Verifier] Reconstructed challenges and query indices from proof commitments.");
        (betas, query_indices)
    }

    /// Verifies a single query decommitment.
    fn verify_query(
        &self,
        proof: &FriProof,
        query_idx: usize,
        betas: &[FE],
        generator: &FE,
        decommitment: &QueryDecommitment,
    ) -> Result<(), FriError> {
        // Step 1: Verify the Merkle proofs for each layer's evaluations.
        self.verify_merkle_paths(proof, query_idx, decommitment)?;

        // Step 2: Verify the folding consistency across all layers.
        self.verify_folding_consistency(proof, query_idx, decommitment, betas, generator)?;

        Ok(())
    }

    /// Verifies that all evaluations in a decommitment are valid against the layer commitments.
    fn verify_merkle_paths(
        &self,
        proof: &FriProof,
        query_idx: usize,
        decommitment: &QueryDecommitment,
    ) -> Result<(), FriError> {
        let mut current_idx = query_idx;

        for i in 0..proof.layer_commitments.len() {
            let domain_size = self.params.domain.len() >> i;
            let sym_idx = (current_idx + domain_size / 2) % domain_size;
            let commitment = &proof.layer_commitments[i];

            // Verify proof for f(x)
            let proof_path = Proof {
                merkle_path: decommitment.layer_auth_paths[i].clone(),
            };
            if !proof_path.verify::<FriBackend>(
                commitment,
                current_idx,
                &decommitment.layer_evaluations[i],
            ) {
                return Err(FriError::InvalidMerkleProof);
            }

            // Verify proof for f(-x)
            let proof_path_sym = Proof {
                merkle_path: decommitment.layer_auth_paths_sym[i].clone(),
            };
            if !proof_path_sym.verify::<FriBackend>(
                commitment,
                sym_idx,
                &decommitment.layer_evaluations_sym[i],
            ) {
                return Err(FriError::InvalidMerkleProof);
            }

            println!(
                "  > Layer {}: Merkle proofs valid for indices {} and {}",
                i, current_idx, sym_idx
            );
            current_idx %= (domain_size / 2).max(1);
        }
        Ok(())
    }

    /// Checks that the folding from layer `i` to `i+1` was done correctly.
    fn verify_folding_consistency(
        &self,
        proof: &FriProof,
        query_idx: usize,
        decommitment: &QueryDecommitment,
        betas: &[FE],
        generator: &FE,
    ) -> Result<(), FriError> {
        // Start with the claimed evaluation from the *next* layer and work backwards.
        // `claimed_child_evaluation` is the value at layer `i+1` that we are checking.
        let mut claimed_child_evaluation = proof.last_layer_value.clone();

        // Iterate backwards from the second-to-last layer down to the first.
        for i in (0..proof.layer_commitments.len() - 1).rev() {
            // Get the evaluations for f(x) and f(-x) at the current layer `i`.
            let y = &decommitment.layer_evaluations[i];
            let y_sym = &decommitment.layer_evaluations_sym[i];

            // Recompute `x` for the specific query index at this layer's domain size.
            let domain_size = self.params.domain.len() >> i;
            let current_query_idx_in_layer = query_idx % domain_size;
            let g_i = generator.pow(1_u64 << i); // Generator for the i-th domain
            let x = g_i.pow(current_query_idx_in_layer);
            let x_inv = x.inv().unwrap();

            // Re-compute what the folded value should be using the folding formula.
            let two_inv = FE::from(2).inv().unwrap();
            let f_even = (y + y_sym) * &two_inv;
            let f_odd = (y - y_sym) * &two_inv * &x_inv;
            let expected_child_evaluation = &f_even + &betas[i] * &f_odd;

            // Check if our calculation matches the claimed evaluation from the next layer.
            if claimed_child_evaluation != expected_child_evaluation {
                return Err(FriError::InconsistentFolding {
                    layer: i,
                    expected: expected_child_evaluation.representative().to_hex(),
                    got: claimed_child_evaluation.representative().to_hex(),
                });
            }

            println!("  > Layer {}->{}: Folding is consistent.", i, i + 1);

            // For the next iteration, the "child" becomes the current evaluation.
            claimed_child_evaluation = y.clone();
        }

        Ok(())
    }

    /// Samples a random index from the transcript.
    fn sample_index(&mut self, max_value: usize) -> usize {
        // Use 8 bytes from the transcript for a u64, then get a value in range.
        let sample_bytes: [u8; 8] = self.transcript.sample()[..8].try_into().unwrap();
        (u64::from_be_bytes(sample_bytes) % max_value as u64) as usize
    }
}
</file>

<file path="4_air_constraints_design/src/arithmetization.rs">
// ================================================================================================
// PART 2: ARITHMETIZATION
// ================================================================================================
// Arithmetization is the process of converting the execution trace into a set of polynomial
// constraints. If the constraints hold, the computation was performed correctly.

use lambdaworks_math::fft::cpu::roots_of_unity::{
    get_powers_of_primitive_root, get_powers_of_primitive_root_coset,
};
use lambdaworks_math::field::traits::{IsFFTField, RootsConfig};
use lambdaworks_math::polynomial::Polynomial;

use crate::{F, FE};

/// Holds the polynomials and domains related to the arithmetized trace.
pub struct Arithmetization {
    pub trace_length: usize,
    pub domain: Vec<FE>,
    pub domain_generator: FE,
    pub trace_poly: Polynomial<FE>,
    // The evaluations of the constraint polynomials over the LDE domain.
    // We store the evaluations directly to avoid interpolating and then re-evaluating,
    // which is more efficient.
    pub boundary_constraint_poly_lde: Vec<FE>,
    pub transition_constraint_poly_lde: Vec<FE>,
    // The domain used for low-degree extension (LDE).
    pub lde_domain: Vec<FE>,
}

impl Arithmetization {
    /// Performs the arithmetization of the execution trace.
    pub fn new(trace: &[FE], blowup_factor: usize) -> Self {
        println!("\n-- STEP 2: ARITHMETIZATION --------------------------------------");
        println!("The Prover transforms the execution trace into polynomial constraints.");

        let trace_length = trace.len();
        assert!(
            trace_length.is_power_of_two(),
            "Trace length must be a power of two for FFT-based interpolation."
        );

        // 1. Define the evaluation domain D_TRACE = {g^0, g^1, ..., g^(n-1)}.
        // This domain corresponds to the steps of our computation.
        let root_order = trace_length.trailing_zeros() as u64;
        let domain_generator = F::get_primitive_root_of_unity(root_order).unwrap();

        let domain =
            get_powers_of_primitive_root::<F>(root_order, trace_length, RootsConfig::Natural)
                .unwrap();

        // 2. Interpolate the trace over D_TRACE to get the trace polynomial t(x).
        // This creates a single polynomial whose evaluations at the domain points match the trace.
        // i.e., t(g^i) = trace[i] for i in [0, n-1].
        let trace_poly = Polynomial::interpolate_fft::<F>(trace).unwrap();
        println!(
            "  [2.1] Interpolated trace of {} elements into trace polynomial t(x) of degree {}.",
            trace_length,
            trace_poly.degree()
        );

        // 3. Define the LDE (Low-Degree Extension) Domain.
        // We evaluate our polynomials on a much larger domain to prevent a dishonest prover
        // from creating a fake polynomial that matches the constraints only on the small domain.
        let lde_domain_size = trace_length * blowup_factor;
        let lde_root_order = lde_domain_size.trailing_zeros();
        let lde_domain = get_powers_of_primitive_root_coset(
            lde_root_order as u64,
            lde_domain_size,
            &FE::from(3), /* A coset offset prevents zeroifiers evaluations equal to 0 (this
                           * would result in division by 0). */
        )
        .unwrap();

        // 4. Evaluate the trace polynomial on the LDE domain.
        // These evaluations, t_lde = {t(x) | x ∈ LDE_domain}, are what the Prover commits to.
        let trace_poly_lde = trace_poly.evaluate_slice(&lde_domain);

        // 5. Boundary Constraints: Ensure the computation starts and ends correctly.
        // Constraint: t(x) must be 1 at the first two steps (g^0 and g^1).
        // Polynomial form: B(x) = (t(x) - I(x)) / Z_B(x), where:
        // - I(x) is a polynomial that evaluates to 1 at g^0 and g^1.
        // - Z_B(x) = (x - g^0)(x - g^1) is a zerofier polynomial.
        // B(x) will be a polynomial (i.e., division is clean) iff the constraints hold.
        println!("  [2.2] Evaluating boundary constraints on the LDE domain...");
        let boundary_constraint_poly_lde = {
            let boundary_interpolant =
                Polynomial::interpolate(&[domain[0], domain[1]], &[FE::one(), FE::one()]).unwrap();
            let boundary_zerofier_poly = Polynomial::new(&[-domain[0], FE::one()])
                * Polynomial::new(&[-domain[1], FE::one()]);

            let numerator_lde = trace_poly_lde
                .iter()
                .zip(&lde_domain)
                .map(|(t_eval, x)| t_eval - boundary_interpolant.evaluate(x))
                .collect::<Vec<_>>();
            let denominator_lde = boundary_zerofier_poly.evaluate_slice(&lde_domain);

            let mut denominator_inv_lde = denominator_lde;
            FE::inplace_batch_inverse(&mut denominator_inv_lde).unwrap();

            numerator_lde
                .iter()
                .zip(denominator_inv_lde.iter())
                .map(|(num, den_inv)| num * den_inv)
                .collect::<Vec<_>>()
        };

        // 6. Transition Constraints: Ensure each step correctly follows from the previous ones.
        // Constraint: For Fibonacci, t(g^2 * x) = t(g * x) + t(x).
        // This must hold for all steps except the last two (where the next state is undefined).
        // Polynomial form: T(x) = (t(g^2 * x) - t(g * x) - t(x)) / Z_T(x), where:
        // - The numerator is the Fibonacci relation.
        // - Z_T(x) = (x^n - 1) / ((x - g^{n-2})(x - g^{n-1})) is the zerofier.
        // T(x) will be a polynomial iff the transition is valid for every step.
        println!("  [2.3] Evaluating transition constraints on the LDE domain...");
        let transition_constraint_poly_lde = {
            let trace_lde_g = trace_poly.evaluate_slice(
                &lde_domain
                    .iter()
                    .map(|x| x * domain_generator)
                    .collect::<Vec<_>>(),
            );
            let trace_lde_g2 = trace_poly.evaluate_slice(
                &lde_domain
                    .iter()
                    .map(|x| x * domain_generator.square())
                    .collect::<Vec<_>>(),
            );
            let numerator_lde = trace_lde_g2
                .iter()
                .zip(trace_lde_g.iter())
                .zip(trace_poly_lde.iter())
                .map(|((t_g2, t_g), t)| t_g2 - t_g - t)
                .collect::<Vec<_>>();

            // The zerofier Z_T(x) vanishes on all points of the trace domain except the
            // last two, where the transition constraint isn't supposed to hold.
            let transition_exemptions_poly =
                (Polynomial::new(&[-domain[trace_length - 2], FE::one()]))
                    * (Polynomial::new(&[-domain[trace_length - 1], FE::one()]));

            let mut exemptions_inv_lde = transition_exemptions_poly.evaluate_slice(&lde_domain);
            FE::inplace_batch_inverse(&mut exemptions_inv_lde).unwrap();

            // Z_T(x) = (x^n - 1) * Z_exemptions(x)^-1
            let denominator_lde = lde_domain
                .iter()
                .zip(exemptions_inv_lde.iter())
                .map(|(x, inv_exemption)| (x.pow(trace_length) - FE::one()) * inv_exemption)
                .collect::<Vec<_>>();

            let mut denominator_inv_lde = denominator_lde;
            FE::inplace_batch_inverse(&mut denominator_inv_lde).unwrap();

            numerator_lde
                .iter()
                .zip(denominator_inv_lde.iter())
                .map(|(num, den_inv)| num * den_inv)
                .collect::<Vec<_>>()
        };

        Self {
            trace_length,
            domain,
            domain_generator,
            trace_poly,
            boundary_constraint_poly_lde,
            transition_constraint_poly_lde,
            lde_domain,
        }
    }
}
</file>

<file path="4_air_constraints_design/Cargo.toml">
[package]
name = "air_constraints_design"
edition.workspace = true
version.workspace = true
readme.workspace = true

[dependencies]
lambdaworks-math.workspace = true
</file>

<file path=".gitignore">
# Added by cargo

target
proptest-regressions
</file>

<file path="rust-toolchain.toml">
# Specifies the Rust toolchain configuration for the project.
[toolchain]

# Sets the toolchain to the stable release channel.
# This ensures that builds are reproducible and use the latest stable version of Rust.
channel = "nightly-2025-04-06"

# Lists the components to be installed with the toolchain.
# "rustfmt" is the standard Rust code formatter. Including it here ensures
# that all developers have it available for consistent code styling.
components = ["rustfmt"]
</file>

<file path="3_polynomial_commitment_scheme/Cargo.toml">
[package]
name = "polynomial_commitment_scheme"
edition.workspace = true
version.workspace = true
readme.workspace = true

[dependencies]
lambdaworks-math.workspace = true
lambdaworks-crypto.workspace = true
hex.workspace = true
</file>

<file path="4_air_constraints_design/README.md">
# **Chapter 4: Algebraic Intermediate Representation (AIR) and Constraint Design**

**Abstract:** This chapter delves into the design of Algebraic Intermediate Representations (AIR), the formal language used to express computational integrity claims in systems like STARKs. Using the Fibonacci sequence as a guiding example, we walk through the process of arithmetization, converting a discrete execution trace into a set of algebraic constraints over a finite field. We meticulously construct boundary and transition constraints, unifying them into a single composition polynomial. The lecture then addresses the critical challenge of linking the prover's claims, explaining the necessity of an out-of-domain check to prevent spoofing and introducing the DEEP composition polynomial as an elegant mechanism for batch-verifying multiple evaluation claims. Finally, we analyze the final spot-check that anchors the entire proof to the initial commitments, providing a holistic and secure protocol.

**Learning Objectives:** Upon completion of this chapter, you will be able to:

- Explain the role of an execution trace and an AIR in converting a computational claim into a verifiable algebraic statement.
- Design and formulate boundary and transition constraints for a given state transition function using polynomials.
- Construct a composition polynomial to probabilistically unify multiple constraints into a single low-degree assertion.
- Articulate why an out-of-domain check is essential for soundness and how the DEEP composition polynomial is used to batch-verify evaluation proofs.
- Describe the final verification step that connects the DEEP proof back to the original trace and composition polynomial commitments.

---

### **Part 1: From Computation to Verifiable Claims**

In the domain of cryptographic proof systems, our primary objective is to verify the integrity of a computation without re-executing it. Systems like STARKs provide a framework to achieve this. The process begins with translating a computational task into a format amenable to mathematical proof—an **execution trace**.

An execution trace is a structured table where each row represents the state of a machine at a discrete time step. The prover, who performed the computation, asserts a **computational claim**: that their execution trace is valid. For this claim to be verifiable, the rules governing the computation's evolution must be expressed as a set of mathematical constraints. This formalization of constraints is known as an **Algebraic Intermediate Representation (AIR)**.

#### **The Computational Claim: A Fibonacci Sequence**

Let's consider the task of generating the `N`-th term of a Fibonacci-like sequence over a finite field `F`. The sequence `(a_i)` for `i >= 0` is defined by:

- $`a_0 = 1`$
- $`a_1 = 1`$
- $`a_{n+2} = a_{n+1} + a_n`$ for all $`n \ge 0`$

The prover's claim is that they have correctly computed the first $`2^n`$ elements. Their evidence is an execution trace, a single-column table of $`2^n`$ rows:

| Step (`i`) | Trace Column (`a_i`) |
| :--------: | :------------------: |
|     0      |        `a_0`         |
|     1      |        `a_1`         |
|     2      |        `a_2`         |
|    ...     |         ...          |
|  `2^n-1`   |     `a_{2^n-1}`      |

The verifier must confirm this trace adheres to the Fibonacci rules without inspecting every row.

---

### **Part 2: Arithmetization and Constraint Design**

The core of a STARK system is **arithmetization**, where we transform the discrete trace into a continuous algebraic object—a polynomial.

#### **The Trace Polynomial**

We select a multiplicative subgroup $`D_S`$ of `F` with order $`2^n`$, generated by a primitive $`2^n`$-th root of unity, `g`. Thus,

$$
D_S = \{g^0, g^1, \dots, g^{2^n-1}\}
$$

We define the **trace polynomial**, $`t(x)`$, as the unique polynomial of degree less than $`2^n`$ that interpolates the trace values over $`D_S`$:

$$
t(g^i) = a_i \quad \text{for } i \in \{0, 1, \dots, 2^n-1\}
$$

The computational claim now becomes a set of algebraic properties that $`t(x)`$ must satisfy.

#### **Boundary Constraint Design**

Boundary constraints enforce specific states at known points. For our sequence, $`a_0 = 1`$ and $`a_1 = 1`$, which translates to:

$$
t(g^0) = 1 \quad \text{and} \quad t(g^1) = 1
$$

We enforce this by constructing a **boundary constraint polynomial**, $`B(x)`$. The principle is that if the constraints hold, $`B(x)`$ must be a true polynomial, not a rational function.

$$
B(x) = \frac{t(x) - P_B(x)}{(x-g^0)(x-g^1)}
$$

Here, $`P_B(x)`$ is a polynomial that interpolates the boundary values (1 and 1) at the boundary points (`g^0` and `g^1`). The denominator, the **zerofier**, is a polynomial that is zero at exactly these points. This construction ensures that if $`t(x)`$ matches $`P_B(x)`$ at the boundaries, the division is exact and $`B(x)`$ is a valid polynomial.

#### **Transition Constraint Design**

Transition constraints define the state evolution rule: $`a_{i+2} = a_{i+1} + a_i`$. This must hold for all valid steps, i.e., $`i`$ in $`\{0, \dots, 2^n-3\}`$. In polynomial terms, this becomes:

$$
t(x \cdot g^2) - t(x \cdot g) - t(x) = 0 \quad \text{for all } x \in \{g^0, \dots, g^{2^n-3}\}
$$

The **transition constraint polynomial**, $`C(x)`$, is formed similarly, where the denominator $`Z_T(x)`$ is a zerofier that vanishes on the domain where the constraint must apply:

$$
C(x) = \frac{t(x \cdot g^2) - t(x \cdot g) - t(x)}{Z_T(x)}
$$

where

$$
Z_T(x) = \prod_{i=0}^{2^n-3} (x-g^i)
$$

---

### **Part 3: The Composition Polynomial and the Out-of-Domain Check**

To avoid multiple proofs, we combine all constraints into a single **composition polynomial**, $`H(x)`$, using random challenges $`\beta_1, \beta_2`$ from the verifier:

$$
H(x) = \beta_1 \cdot B(x) + \beta_2 \cdot C(x)
$$

The prover's claim is now reduced to a single, powerful assertion: **`H(x)` is a low-degree polynomial.** The prover will use the FRI protocol to prove this.

#### **The First Challenge: Proving the Honesty of `H(x)`**

A crucial step is to ensure that the polynomial $`H(x)`$ the prover commits to is the one correctly constructed from the trace polynomial $`t(x)`$. A verifier needs to check the formula for $`H(x)`$ at some point. But which point?

> #### Deep Dive: The Flaw of In-Domain Checks
>
> If the verifier chose an in-domain point where a constraint applies (e.g., $`x_0 = g^i`$ for $`i < 2^n-2`$), they would encounter an indeterminate form $`0/0`$.
>
> Consider the transition constraint $`C(x_0)`$. If the prover is honest, the numerator $`t(x_0 \cdot g^2) - t(x_0 \cdot g) - t(x_0)`$ will be 0. However, the denominator $`Z_T(x_0)`$ is _also_ 0 by definition of the zerofier.
>
> A cheating prover could commit to completely unrelated polynomials for $`t(x)`$ and $`H(x)`$, and this check would still pass trivially. It reveals nothing about the relationship between the committed polynomials and is easily spoofed.

The solution is the **out-of-domain check**. The verifier chooses a random point `z` from `F` that is _not_ in the trace domain `D_S`. This ensures that the zerofier denominators in `B(z)` and `C(z)` are non-zero. The verifier then challenges the prover to provide the evaluations needed to compute `H(z)`:

- $`H(z)`$
- $`t(z)`$
- $`t(z \cdot g)`$
- $`t(z \cdot g^2)`$

The verifier uses the provided trace evaluations to locally compute what $`H(z)`$ _should_ be and compares it to the value supplied by the prover. By the Schwartz-Zippel Lemma, if this check passes for a random `z`, it is highly probable that the prover's committed $`H(x)`$ and $`t(x)`$ are related by the correct formula _as polynomials_, not just at a few specific points.

This, however, introduces a new set of claims. The verifier has received these evaluations, but has no guarantee they are authentic. The prover must now prove that the values they sent are genuine evaluations of the polynomials they initially committed to.

---

### **Part 4: The DEEP Composition Polynomial and Final Verification**

The prover must now substantiate multiple claims simultaneously:

1.  **The Main Claim:** `H(x)` is a low-degree polynomial.
2.  **The Opening Claims:** The provided evaluations $`H(z), t(z), t(z \cdot g), t(z \cdot g^2)`$ are correct.

Proving that an evaluation $`P(z) = y`$ is correct for a committed polynomial $`P(x)`$ is equivalent to proving that the quotient $`\frac{P(x) - y}{x - z}`$ is a low-degree polynomial. Performing a separate FRI proof for each of these claims would be highly inefficient.

The **DEEP Composition Polynomial** provides an elegant solution by batching all these polynomiality proofs into one. The verifier supplies a new set of random challenges $`\gamma_i`$ from `F`, and the prover constructs:

$$
\begin{align*}
\text{Deep}(x) = \gamma_1 \cdot \frac{H(x) - H(z)}{x - z} + \gamma_2 \cdot \frac{t(x) - t(z)}{x - z} + \gamma_3 \cdot \frac{t(x) - t(z \cdot g)}{x - z \cdot g} + \gamma_4 \cdot \frac{t(x) - t(z \cdot g^2)}{x - z \cdot g^2}
\end{align*}
$$

The prover then executes a single FRI protocol to prove that `Deep(x)` is a low-degree polynomial.

> #### Deep Dive: The Final Link - The `Deep(x₀)` Check
>
> The FRI proof establishes that the prover knows _some_ low-degree polynomial, let's call it $`D_{\text{committed}}(x)`$. The final, critical link is to ensure that this is the _correct_ `Deep(x)` constructed from the _original_ committed polynomials, `[t]` and `[H]`. This is achieved by performing a spot-check at a random **in-domain** point `x₀`, which is one of the query points from the FRI protocol.
>
> 1.  **Prover's Role:** For the query point `x₀`, the prover provides:
>
>     - The evaluation `Deep(x₀)` along with its FRI proof.
>     - The evaluations `H(x₀)` and `t(x₀)` along with their **Merkle proofs** tying them back to the original commitments `[H]` and `[t]`.
>
> 2.  **Verifier's Role:** The verifier executes a crucial three-step verification:
>     - **Anchor the Ingredients:** The verifier first validates the Merkle proofs for `H(x₀)` and `t(x₀)`. This step makes these values cryptographically trustworthy anchors to the initial state of the protocol.
>     - **Reconstruct Locally:** Using these trusted values `H(x₀), t(x₀)`, along with the out-of-domain evaluations at `z` and the random challenges `γ_i`, the verifier calculates the value that `Deep(x₀)` _should_ have according to its public formula.
>     - **Compare and Conclude:** The verifier compares this locally computed value with the value of `Deep(x₀)` opened from the FRI proof. If they match, the loop is closed.
>
> This single check at `x₀` proves that the polynomial demonstrated to be low-degree by FRI is the same one that correctly binds the out-of-domain evaluations to the initially committed trace.

### **Conclusion**

The STARK protocol is a sequence of elegant reductions. A complex computational trace is reduced to a set of polynomial constraints. These are combined into a single composition polynomial, `H(x)`. The claim that `H(x)` is low-degree and correctly constructed is then further reduced into a series of opening claims. Finally, the **DEEP Composition Polynomial** batches these opening claims into a single object, whose integrity is proven with one FRI protocol. The check at an in-domain point `x₀` serves as the final anchor, binding the entire intricate web of claims back to the original commitments, thus providing a holistic and secure proof of computational integrity.

---

**Author:** [Okm165](https://github.com/Okm165) | [@bartolomeo_diaz](https://x.com/bartolomeo_diaz)
</file>

<file path="2_fast_polynomial_arithmetic/README.md">
# Chapter 2: The Fast Fourier Transform for Polynomial Multiplication

**Abstract:** This chapter provides an introduction to the Fast Fourier Transform (FFT) as a high-performance algorithm for polynomial multiplication. We begin by examining the computational complexity of polynomial operations within their standard coefficient-basis representation, identifying the $`\Theta(n^2)`$ complexity of convolution as a significant bottleneck. An alternative, the point-value representation, is introduced, which reduces multiplication to a linear-time operation.

The core of this chapter frames the FFT as an efficient algorithm for mediating a change of basis between the coefficient and point-value domains. This is achieved by selecting a specific set of evaluation points, the complex roots of unity, which imbue the transformation matrix with a recursive structure. We formally derive the Cooley-Tukey radix-2 FFT algorithm, analyzing its $`\Theta(n \log n)`$ complexity via its divide-and-conquer structure. We then deconstruct the mechanics of its iterative implementation, elucidating the necessity of the butterfly operation and the bit-reversal permutation as direct consequences of the recursive algorithm's data flow, answering key questions about their structure and implementation. Finally, we prove that the inverse transform, necessary for interpolation, possesses a structure nearly identical to the forward transform, allowing for its computation with the same algorithmic efficiency.

**Learning Objectives:** Upon completion of this chapter, you will be able to:

1.  Explain why polynomial multiplication is computationally expensive ($`\Theta(n^2)`$) in the coefficient basis and efficient ($`\Theta(n)`$) in the point-value basis.
2.  Define the Discrete Fourier Transform (DFT) as a change of basis using the complex roots of unity, and articulate the algebraic properties that make these points ideal for efficient computation.
3.  Derive the $`\Theta(n \log n)`$ complexity of the Fast Fourier Transform (FFT) from its recursive, divide-and-conquer structure.
4.  Describe the structure of an iterative FFT, including the role of butterfly operations and the bit-reversal permutation.
5.  Explain how the Inverse FFT leverages the same algorithmic core to efficiently convert a polynomial from its point-value representation back to its coefficient representation.

---

## Part 1: Polynomial Representations and Computational Trade-offs

The efficiency of algorithms is intrinsically linked to the underlying representation of the data they manipulate. For univariate polynomials, the choice of representation dictates the computational complexity of fundamental operations.

#### The Coefficient Representation

A univariate polynomial $`A(x)`$ of degree-bound $`n`$ is conventionally defined by a vector of $`n`$ coefficients, $`a = (a_0, a_1, \dots, a_{n-1})`$, in the standard monomial basis:

```math
A(x) = \sum_{j=0}^{n-1} a_j x^j
```

In this basis, adding two polynomials $`A(x)`$ and $`B(x)`$ is a component-wise vector addition of their coefficient vectors, a $`\Theta(n)`$ operation. However, multiplying them to get $`C(x) = A(x) \cdot B(x)`$ yields a polynomial whose coefficients are determined by the **convolution** of the input coefficient vectors, denoted $`c = a * b`$:

```math
c_k = \sum_{j=0}^{k} a_j b_{k-j}
```

#### The Point-Value Representation

A fundamental theorem of algebra establishes that a unique polynomial of degree-bound $`n`$ is determined by $`n`$ distinct point-value pairs. Thus, an alternative representation for $`A(x)`$ is a set $`\{(x_0, y_0), (x_1, y_1), \dots, (x_{n-1}, y_{n-1})\} `$, where all $`x_i`$ are distinct and $`y_i = A(x_i)`$.

Within this representation, operations on polynomials evaluated at an identical set of points are computationally efficient. Addition $`C(x) = A(x) + B(x)`$ corresponds to $`(x_i, y_i + y'_i)`$, and multiplication $`C(x) = A(x) \cdot B(x)`$ corresponds to $`(x_i, y_i \cdot y'_i)`$. Both are $`\Theta(n)`$ operations. Note that for multiplication, the degree of the product (up to $`2n-2`$) requires that the initial evaluation be performed on at least $`2n-1`$ points.

This efficiency, however, introduces a critical trade-off. The primary drawback of the point-value representation is the difficulty of evaluating the polynomial at an arbitrary point `z` that is not among the initial set `{x_i}`. Such an operation requires an interpolation algorithm (like Lagrange interpolation) that must process all `n` known points, making it far more costly than the simple `O(n)` evaluation (via Horner's method) possible in the coefficient basis.

This dichotomy in complexities suggests a three-step strategy for fast polynomial multiplication:

1.  **Evaluation:** Transform the input polynomials from the coefficient basis to a point-value representation at $`N \ge 2n-1`$ points.
2.  **Pointwise Product:** Perform the $`\Theta(N)`$ multiplication in the point-value domain.
3.  **Interpolation:** Transform the resulting product polynomial back to the coefficient basis.

The asymptotic complexity of this strategy is dominated by the evaluation and interpolation steps. A naive evaluation at $`N`$ points requires $`\Theta(N \cdot n)`$ time, offering no asymptotic advantage. This motivates the central problem: **the search for a specific set of evaluation points that facilitates a sub-quadratic change of basis.**

---

## Part 2: The Discrete Fourier Transform as a Change of Basis

The conversion between the coefficient and point-value representations can be formalized through the lens of linear algebra.

#### The Vandermonde Matrix Formulation

The evaluation of a degree-bound $`n`$ polynomial $`A(x)`$ at $`n`$ distinct points $`x_0, \dots, x_{n-1}`$ constitutes a linear transformation, expressible as the matrix-vector product $`y = V \cdot a`$:

```math
\begin{bmatrix} y_0 \\ y_1 \\ \vdots \\ y_{n-1} \end{bmatrix} = \begin{bmatrix} 1 & x_0 & x_0^2 & \dots & x_0^{n-1} \\ 1 & x_1 & x_1^2 & \dots & x_1^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \dots & x_{n-1}^{n-1} \end{bmatrix} \begin{bmatrix} a_0 \\ a_1 \\ \vdots \\ a_{n-1} \end{bmatrix}
```

The matrix $`V`$, whose entries are $`V_{jk} = x_j^k`$, is a **Vandermonde matrix**. Evaluation is a projection via $`V`$; interpolation is the inverse operation, $`a = V^{-1} \cdot y`$. The invertibility of $`V`$ is guaranteed if and only if the evaluation points $`x_j`$ are distinct. Our objective is to select these points such that multiplication by $`V`$ and $`V^{-1}`$ can be executed rapidly.

#### The Complex Roots of Unity

The requisite structure is found by choosing the evaluation points to be the **n-th complex roots of unity**. An $`n`$-th root of unity is a complex number $`\omega`$ satisfying $`\omega^n = 1`$. The $`n`$ distinct roots are given by:

```math
\omega_n^k = e^{2\pi i k / n} \quad \text{for } k = 0, 1, \dots, n-1
```

These roots form a cyclic group under multiplication and exhibit properties essential for an efficient recursive algorithm, most notably the Halving Lemma.

> **Lemma (Halving):** For any even integer $`n > 0`$, the set of squares of the $`n`$-th roots of unity is identical to the set of $`(n/2)`$-th roots of unity, where each root appears twice.
>
> _Proof sketch:_ Squaring $`\omega_n^k`$ yields $`(\omega_n^k)^2 = (e^{2\pi i k / n})^2 = e^{2\pi i (2k) / n} = e^{2\pi i k / (n/2)} = \omega_{n/2}^k`$. Further, $`(\omega_n^{k+n/2})^2 = (\omega_n^k \cdot \omega_n^{n/2})^2 = (\omega_n^k)^2 \cdot (-1)^2 = \omega_{n/2}^k`$. Thus, both $`\omega_n^k`$ and $`\omega_n^{k+n/2}`$ square to the same $`(n/2)`$-th root of unity.

#### The Discrete Fourier Transform (DFT)

When the evaluation points are the $`n`$-th roots of unity, the Vandermonde matrix becomes the **DFT matrix, $`F_n`$**, and the transformation $`y = F_n \cdot a`$ is the **Discrete Fourier Transform**. The FFT is the algorithm that computes this product efficiently.

---

## Part 3: The Cooley-Tukey Radix-2 FFT Algorithm

The canonical FFT algorithm, attributed to Cooley and Tukey, employs a divide-and-conquer strategy. Assuming $`n`$ is a power of 2, a polynomial $`A(x)`$ is decomposed based on the parity of its coefficient indices into $`A_{\text{even}}(y)`$ and $`A_{\text{odd}}(y)`$, where $`y=x^2`$:

```math
A(x) = A_{\text{even}}(x^2) + x \cdot A_{\text{odd}}(x^2)
```

When evaluating $`A(x)`$ at the $`n`$-th roots of unity, $`\omega_n^k`$, the Halving Lemma ensures that $`A_{\text{even}}`$ and $`A_{\text{odd}}`$ need only be evaluated at $`(\omega_n^k)^2 = \omega_{n/2}^k`$. This reduces a problem of size $`n`$ to two subproblems of size $`n/2`$. The recurrence relation for this process is $`T(n) = 2T(n/2) + \Theta(n)`$, which, by the Master Theorem, solves to a running time of **$`\Theta(n \log n)`$**. The recursive combination step forms the heart of the algorithm.

> #### Deep Dive: Deriving the Butterfly Formulas
>
> The two-output shape of the butterfly is not arbitrary; it is a direct result of evaluating our core identity $`A(x) = A_{\text{even}}(x^2) + x \cdot A_{\text{odd}}(x^2)`$ at two specific points, $`\omega_n^k`$ and $`\omega_n^{k+n/2}`$, which share the same square. Let $`y_{\text{even}}[k]`$ be the result of evaluating $`A_{\text{even}}`$ at $`\omega_{n/2}^k`$, and $`y_{\text{odd}}[k]`$ be the result for $`A_{\text{odd}}`$.
>
> 1.  **Evaluate at $`x = \omega_n^k`$:**
>
> ```math
> y[k] = A(\omega_n^k) = A_{\text{even}}((\omega_n^k)^2) + \omega_n^k \cdot A_{\text{odd}}((\omega_n^k)^2)
> ```
>
> By the Halving Lemma, $`(\omega_n^k)^2 = \omega_{n/2}^k`$. Substituting the results from the recursive calls:
>
> ```math
> y[k] = y_{\text{even}}[k] + \omega_n^k \cdot y_{\text{odd}}[k]
> ```
>
> 2.  **Evaluate at $`x = \omega_n^{k+n/2}`$:**
>
> ```math
> y[k+n/2] = A(\omega_n^{k+n/2}) = A_{\text{even}}((\omega_n^{k+n/2})^2) + \omega_n^{k+n/2} \cdot A_{\text{odd}}((\omega_n^{k+n/2})^2)
> ```
>
> The Halving Lemma also gives $`(\omega_n^{k+n/2})^2 = \omega_{n/2}^k`$. And we know $`\omega_n^{k+n/2} = \omega_n^k \cdot \omega_n^{n/2} = -\omega_n^k`$. Substituting:
>
> ```math
> y[k+n/2] = y_{\text{even}}[k] - \omega_n^k \cdot y_{\text{odd}}[k]
> ```
>
> The algorithm produces two distinct outputs because a single pair of subproblem results, $`y_{\text{even}}[k]`$ and $`y_{\text{odd}}[k]`$, contains all the information needed to compute the final DFT at two different output indices, $`k`$ and $`k+n/2`$. This two-for-one computation is the source of the FFT's efficiency. The twiddle factor $`\omega_n^k`$ is applied only to the odd branch because it arises from the explicit $`x`$ multiplier in the decomposition $`A_{\text{even}}(x^2) + x \cdot A_{\text{odd}}(x^2)`$. The $`A_{\text{even}}`$ term is a function of $`x^2`$ only and is thus "left untouched," while the $`A_{\text{odd}}`$ term is scaled by $`x`$, which becomes $`\omega_n^k`$ upon evaluation. Swapping this choice would violate the algebraic identity.

---

## Part 4: The Structure of the Iterative FFT

While the recursive formulation is pedagogically clear, practical implementations are typically iterative to avoid the overhead of function calls. The iterative structure is a "bottom-up" reversal of the recursive decomposition.

#### The Butterfly Operation and Twiddle Factor Indexing

The butterfly is the atomic computational unit of the iterative FFT. An $`n`$-point FFT consists of $`\log_2(n)`$ stages. Stage $`s`$ (from $`1`$ to $`\log_2(n)`$) computes DFTs of size $`2^s`$ by combining pairs of DFTs of size $`2^{s-1}`$.

> #### Deep Dive: Calculating Twiddle Factor Exponents
>
> In an iterative implementation, the twiddle factor $`\omega_n^m`$ used in a butterfly depends on the stage and position. In stage $`s`$, the butterflies combine elements that are $`2^{s-1}`$ positions apart. The twiddle factor exponent cycles through $`0, 1, 2, \dots`$ within each block of size $`2^s`$.
>
> **Example:** For a 16-point FFT ($`n=16`$), Stage 3 ($`s=3`$) combines DFTs of size 4 to create DFTs of size 8.
>
> - The butterfly size is $`2^3 = 8`$.
> - The twiddle factors used are $`\omega_8^0, \omega_8^1, \omega_8^2, \omega_8^3`$. Note that $`\omega_8^k = \omega_{16}^{2k}`$.
> - The butterfly starting at index $`i=0`$ combines `array[0]` and `array[4]` using $`\omega_8^0`$.
> - The butterfly starting at index $`i=1`$ combines `array[1]` and `array[5]` using $`\omega_8^1`$.
> - The butterfly starting at index $`i=2`$ combines `array[2]` and `array[6]` using $`\omega_8^2`$.
> - The butterfly starting at index $`i=3`$ combines `array[3]` and `array[7]` using $`\omega_8^3`$.
> - The butterfly starting at index $`i=8`$ combines `array[8]` and `array[12]` using $`\omega_8^0`$.
>
> The pattern repeats for the next block. The exponent $`m`$ for the twiddle factor $`\omega_{2^s}^m`$ applied to the pair starting at index $`i`$ is simply $`m = i \pmod{2^{s-1}}`$.

#### The Bit-Reversal Permutation

An efficient, in-place iterative FFT requires that the input data be pre-sorted. Tracing the recursive decomposition reveals the necessary ordering. The final, base-case subproblems operate on coefficients whose indices, when traced back through the even/odd splits, correspond to a bit-reversed ordering.

> #### Deep Dive: The Uniqueness and Implementation of Bit-Reversal
>
> **Why bit-reversal?** The invariant that bit-reversal satisfies is this: _it places any two coefficients $`a_i`$ and $`a_j`$ that are destined for the same base-case butterfly (at the deepest level of recursion) adjacent in the initial permuted array_. For the standard "decimation-in-time" FFT, two indices $`i`$ and $`j`$ end up in the same final $`(a_i, a_j)`$ pair if and only if their binary representations differ only in their most significant bit. Bit-reversing these indices makes them differ only in their _least_ significant bit, placing them at adjacent indices $`2k`$ and $`2k+1`$. No other simple permutation achieves this necessary alignment for all pairs simultaneously, allowing for contiguous butterfly operations at every stage.
>
> **Cost vs. Benefit:** The $`\Theta(n)`$ one-time cost of bit-reversal is compared to the $`\Theta(n^2)`$ cost of a naive DFT. The FFT, with its $`\Theta(n \log n)`$ complexity plus bit-reversal, is almost always superior. For very small $`n`$ (e.g., $`n < 32`$), the hidden constant factors in the FFT's butterfly loops and memory access patterns might make a direct, unrolled DFT computation faster. High-performance libraries like FFTW often contain hand-optimized "codelets" for these small sizes and may not perform an explicit bit-reversal pass, instead using hardcoded permutations. The crossover point is highly architecture-dependent, but for any cryptographically relevant $`n`$, the $`\Theta(n)`$ cost is negligible.

---

## Part 5: Interpolation via the Inverse DFT

The final step, converting the point-value product back to coefficients, requires computing $`a = F_n^{-1} \cdot y`$.

#### The Inverse DFT Matrix

The inverse of the DFT matrix $`F_n`$ possesses a highly structured form, a direct consequence of the orthogonality of the roots of unity.

> **Theorem: The Inverse DFT Matrix**
> The inverse of the DFT matrix $`F_n`$ is given by:
>
> ```math
> (F_n^{-1})_{jk} = \frac{1}{n} \cdot \omega_n^{-jk}
> ```
>
> _Proof:_ Let $`P = F_n \cdot F'_n`$, where $`(F'_n)_{jk} = \omega_n^{-jk}`$. The entry $`P_{jk}`$ is:
>
> ```math
> P_{jk} = \sum_{m=0}^{n-1} \omega_n^{jm} \cdot \omega_n^{-mk} = \sum_{m=0}^{n-1} (\omega_n^{j-k})^m
> ```
>
> We examine two cases:
>
> 1.  **If $`j = k`$ (on-diagonal):** The term is $`(\omega_n^0)^m = 1^m = 1`$. The sum is therefore $`\sum_{m=0}^{n-1} 1 = n`$.
>
> 2.  **If $`j \neq k`$ (off-diagonal):** This is a finite geometric series with ratio $`z = \omega_n^{j-k} \neq 1`$. The sum is given by the formula $`(z^n - 1)/(z - 1)`$. The numerator is $`(\omega_n^{j-k})^n = (\omega_n^n)^{j-k} = 1^{j-k} = 1`$. Thus, the sum is $`(1-1)/(z-1) = 0`$.
>
> So, $`P`$ is the matrix $`n \cdot I`$, where $`I`$ is the identity matrix. It follows that $`F_n^{-1} = \frac{1}{n} \cdot F'_n`$.

#### The Inverse FFT Algorithm

This structural identity implies that a separate algorithm for interpolation is unnecessary. The inverse DFT can be computed using the forward FFT algorithm with two modifications:

1.  The principal root of unity $`\omega_n`$ is replaced with its inverse $`\omega_n^{-1}`$.
2.  The final output vector is scaled by a factor of $`1/n`$.

This elegant duality ensures that interpolation is also a **$`\Theta(n \log n)`$** operation, preserving the overall efficiency of the multiplication strategy.

---

The Fast Fourier Transform is an algorithmic cornerstone of computational mathematics and engineering. It resolves the $`\Theta(n^2)`$ complexity of polynomial convolution by providing a $`\Theta(n \log n)`$ method for changing basis between the coefficient and point-value representations. This efficiency is a direct consequence of the profound algebraic and geometric properties of the complex roots of unity. The specific structures of the butterfly operation and the bit-reversal permutation are not arbitrary design choices but are necessary consequences of implementing the algorithm's recursive logic in an efficient, iterative manner. For cryptographic systems and other advanced applications, the FFT is not merely an optimization; it is the enabling technology that makes large-scale, practical implementations computationally feasible.

---

<p align="center">
  <img src="./benches/lines.svg" alt="Benchmarks FFT vs Naive" style="width:100%; max-width:800px;">
</p>

---

### [Chapter 3: Foundations of Polynomial Commitment Schemes with a Focus on FRI](../3_polynomial_commitment_scheme/README.md)

---

**Author:** [Okm165](https://github.com/Okm165) | [@bartolomeo_diaz](https://x.com/bartolomeo_diaz)
</file>

<file path="3_polynomial_commitment_scheme/README.md">
# **Chapter 3: Foundations of Polynomial Commitment Schemes with a Focus on FRI**

**Abstract:** This chapter introduces Polynomial Commitment Schemes (PCS) as a foundational cryptographic primitive essential for modern Zero-Knowledge Proof (ZKP) systems. We will define a PCS and its core properties—binding, succinctness, and evaluation proofs—and explore its role in translating computational integrity claims into verifiable algebraic statements. A key focus is the fundamental dichotomy between schemes requiring a trusted setup and transparent schemes, setting the stage for a detailed examination of the FRI protocol. The chapter then provides a detailed dissection of FRI, establishing its mathematical preliminaries in finite fields and Reed-Solomon codes, and meticulously walking through its Commit, Fold, and Query phases. Finally, we will analyze the security foundations of FRI, explaining its probabilistic soundness, the role of the Fiat-Shamir heuristic in making it practical, and the performance trade-offs inherent in its transparent design.

**Learning Objectives:** Upon completion of this chapter, you will be able to:

- Define a Polynomial Commitment Scheme and its primary operations.
- Explain the core properties of binding, succinctness, and evaluation proofs.
- Describe the role of PCS as a building block in Zero-Knowledge Proofs.
- Detail the Commit, Fold, and Query phases of the FRI protocol.

---

## **Part 1: The Role and Definition of Polynomial Commitment Schemes**

#### **1.1 The Need for Succinct Verification**

The first step in many ZKP systems is **arithmetization**, a process that converts a claim of computational integrity into an equivalent algebraic statement. This is often expressed as an assertion that a particular polynomial, derived from the computation's execution trace, possesses certain properties (e.g., it has a low degree). A Polynomial Commitment Scheme (PCS) is the cryptographic tool that allows a Prover to commit to this polynomial and then prove that it satisfies the required properties, all in a highly efficient and succinct manner.

#### **1.2 Defining a Polynomial Commitment Scheme**

A Polynomial Commitment Scheme is a cryptographic protocol that enables a Prover to commit to a polynomial `P(X)` and later prove evaluations of that polynomial at specific points, without revealing the entire polynomial itself. A PCS typically consists of three core algorithms:

1.  **Commit(`P(X)`) → `c`**: The Prover executes this algorithm to generate a short, fixed-size string `c`, known as the **commitment**, to the polynomial `P(X)`. This commitment acts as a unique and tamper-proof "fingerprint" of the polynomial.
2.  **Open(`P(X)`, `z`) → `π`**: To prove the evaluation `y = P(z)` at a point `z`, the Prover generates a proof `π`. This proof is also typically short.
3.  **Verify(`c`, `z`, `y`, `π`) → {Accept, Reject}**: The Verifier uses this algorithm to check if the proof `π` is valid for the claimed evaluation `y` at point `z`, with respect to the original commitment `c`.

#### **1.3 Core Properties of a PCS**

For a PCS to be secure and useful, it must satisfy several critical properties:

- **Binding:** Once a Prover commits to a polynomial `P(X)` via a commitment `c`, they are cryptographically "bound" to it. It must be computationally infeasible for the Prover to later produce a valid proof for a different polynomial `P'(X)` that also corresponds to the same commitment `c`. This ensures the integrity and immutability of the commitment.
- **Hiding (Optional but Desirable):** The commitment `c` should not reveal any information about the polynomial `P(X)`. An observer seeing only `c` should not be able to deduce the coefficients of `P(X)`.
- **Evaluation Proofs:** The scheme must provide a mechanism for a Prover to prove that `P(z) = y` for any given point `z`. This proof, `π`, allows the Verifier to be convinced of the evaluation's correctness without needing to know the entire polynomial.
- **Succinctness:** Both the commitment `c` and the proof `π` must be significantly smaller than a full description of the polynomial `P(X)`. Furthermore, the `Verify` algorithm should be highly efficient, ideally running in time that is polylogarithmic in the degree of `P(X)`. Succinctness is the key to achieving scalability.

#### **1.4 The Foundational Split: Transparency vs. Trusted Setups**

Not all Polynomial Commitment Schemes are created equal. A fundamental distinction lies in how their public parameters are generated, which gives rise to two major classes of schemes.

- **Trusted Setup Schemes (e.g., KZG):** Some of the most efficient PCS in terms of proof size, such as the Kate-Zaverucha-Goldberg (KZG) scheme, require a **trusted setup**. In a one-time ceremony, a secret random value, `τ` (tau), is generated and then must be destroyed. The security of the entire system relies on this "toxic waste" being permanently forgotten. If `τ` were ever compromised, an attacker could forge invalid proofs, breaking the system's soundness. This trust assumption is a significant social and technical vulnerability.

- **Transparent Schemes (e.g., FRI):** In contrast, **transparent** schemes require no trusted setup. All public parameters are generated from public randomness, typically via a cryptographic hash function. This eliminates the single point of failure associated with a trusted setup, making the system more robust, decentralized, and philosophically aligned with trustless systems. Many transparent schemes, including FRI, also base their security on collision-resistant hash functions, an assumption believed to be resistant to quantum attacks, making them a more "future-proof" choice. The primary trade-off is often performance, as transparent schemes typically have larger proof sizes than their trusted-setup counterparts.

---

## **Part 2: The FRI Protocol: Mechanism and Mathematical Foundations**

#### **2.1 Mathematical Preliminaries**

The FRI protocol operates within the mathematical structure of a **finite field**, also known as a Galois Field `GF(p)`. For FRI, the field `GF(p)` consists of integers `{0, 1, ..., p-1}` where `p` is a large prime number, and all operations are performed modulo `p`.

For efficiency, FRI relies on evaluation domains that are **multiplicative subgroups** of the field, generated by a **primitive n-th root of unity**, `ω`. This element `ω` generates `n` distinct points `{ω^0, ω^1, ..., ω^(n-1)}` that form the evaluation domain `D`. The size of the domain, `n`, is typically a power of two to facilitate the recursive halving in the protocol.

At its heart, FRI is a protocol for proving proximity to a **Reed-Solomon (RS) codeword**. A message (a set of `k` field elements) can be interpreted as the coefficients of a polynomial `P(X)` of degree less than `k`. The RS codeword is the evaluation of `P(X)` at `n` distinct points. Therefore, testing if a set of evaluations corresponds to a low-degree polynomial is equivalent to testing if it is a valid RS codeword.

FRI is not a strict low-degree test; it is a **proof of proximity**. This means it proves that a given set of evaluations is "close" (in Hamming distance) to a valid RS codeword, meaning it differs from some low-degree polynomial in only a small number of positions.

#### **2.2 The FRI Protocol in Detail**

The FRI protocol unfolds in three phases. Imagine a Prover wants to convince a Verifier that a function `f₀`, for which they have committed, is the evaluation of a low-degree polynomial.

**Phase 1: The Commit Phase**

1.  **Initial Commitment:** The Prover starts with the evaluations of their polynomial `f₀` over a large domain `D₀` of size `n`. They build a Merkle tree from these `n` evaluations and send the **Merkle root** to the Verifier. This root is the commitment to `f₀`.

**Phase 2: The Fold Phase (Iterative Degree-Reduction)**

This is the iterative core of the protocol. The goal is to recursively reduce the degree of the polynomial until it becomes a constant.

1.  **The Folding Operation:** A polynomial `f(x)` can be split into its even and odd degree components: `f(x) = f_e(x²) + x · f_o(x²)`. Here, `f_e` and `f_o` are polynomials with roughly half the degree of `f`.
2.  **The Recursive Step:** For each round `i`:
    - The Verifier sends a random challenge value `βᵢ` from the finite field.
    - The Prover uses `βᵢ` to "fold" the previous polynomial `fᵢ₋₁` into a new polynomial `fᵢ` using a random linear combination: `fᵢ(x) = fᵢ₋₁_e(x) + βᵢ · fᵢ₋₁_o(x)`.
    - This new polynomial `fᵢ` has its degree halved. The domain of evaluation also shrinks.
    - The Prover commits to the evaluations of `fᵢ` by building a new Merkle tree and sending its root to the Verifier.
3.  **Final Step:** This process repeats until the final polynomial is reduced to a constant (degree 0). The Prover sends this final constant value directly to the Verifier.

> **Deep Dive: Mathematical Proof of Degree Reduction**
> Let `f(x)` be a polynomial of degree `d-1`. Its even and odd components, `f_e(Y)` and `f_o(Y)`, will have a degree of at most `⌊(d-1)/2⌋`. The new polynomial is `f'(Y) = f_e(Y) + β · f_o(Y)`. Since `deg(f_e)` and `deg(f_o)` are at most `(d-1)/2`, the degree of their linear combination `f'` is also at most `(d-1)/2`. Thus, the degree is effectively halved in each round.

**Phase 3: The Query Phase**

After committing and folding, the Verifier checks that the Prover was honest by issuing random "queries."

1.  **Query Issuance:** The Verifier picks a random leaf index `j` from the original Merkle tree of `f₀`.
2.  **Prover's Response:** For this index, the Prover provides the evaluation `f₀(dⱼ)` and all corresponding evaluations from the subsequent folded layers, along with the **Merkle authentication paths** for each value.
3.  **Verifier's Check:** The Verifier performs two crucial checks:
    - **Merkle Path Verification:** The Verifier uses the authentication paths to confirm that the revealed values are consistent with the Merkle roots received in the commit phase.
    - **Consistency Check:** The Verifier uses the revealed evaluations from consecutive layers (`fᵢ` and `fᵢ₊₁`) and the random challenge `βᵢ₊₁` to check that the folding formula holds true. For a given point `y` in a domain `Dᵢ`, the verifier needs `fᵢ(y)` and `fᵢ(-y)` to compute `fᵢ₊₁(y²)`. If the Prover's revealed value for `fᵢ₊₁(y²)` matches the Verifier's calculation, the check passes for that query.

By performing enough random queries, the Verifier becomes statistically convinced that the original commitment was indeed to a low-degree polynomial.

---

## **Part 3: Security, Transparency, and Practical Considerations**

#### **3.1 The Probabilistic Soundness of FRI**

The security of FRI is not absolute but **probabilistic**. A dishonest Prover has a non-zero, but negligibly small, probability of cheating, known as the **soundness error**. This security relies on randomness.

- **Random Folding Challenges (`βᵢ`):** A dishonest Prover who starts with a function that is "far" from any low-degree polynomial cannot predict the random challenge `βᵢ`. This makes it computationally infeasible for them to craft a folded polynomial that maliciously appears "close" to a low-degree polynomial.
- **Random Queries:** Because the Prover cannot predict which points the Verifier will check, they must be honest across the _entire_ domain. Any inconsistency is highly likely to be exposed by a random query.

The formal argument for FRI's soundness relies on the **Proximity Gap Theorem**, which informally states that if a function is "far" from the set of low-degree polynomials, the randomly folded function will also be "far" from the set of halved-degree polynomials. This guarantees that "farness" (i.e., cheating) is propagated through the rounds and will be detected at the final check, causing the Verifier to reject.

#### **3.2 The Power of Transparency**

As established, FRI's transparency is a compelling feature. By obviating the need for a trusted setup, it provides key advantages:

- **Trust Minimization:** It removes the need to trust participants of a setup ceremony, making the system more secure.
- **Reduced Complexity:** It eliminates the logistical and security challenges of executing a secure setup ceremony.
- **Permissionless Participation:** Anyone can become a Prover or Verifier using only public information.
- **Plausible Post-Quantum Security:** By relying on hash functions, FRI offers stronger long-term security against future quantum computers.

#### **3.3 Making FRI Non-Interactive: The Fiat-Shamir Heuristic**

In its natural form, FRI is an **interactive** protocol. For many applications, like posting a proof to a blockchain, this is impractical. The **Fiat-Shamir heuristic** transforms it into a non-interactive proof by replacing the Verifier's random challenges with the output of a cryptographic hash function.

Instead of waiting for a random `βᵢ` from the Verifier, the Prover computes it themselves by hashing the public transcript up to that point (e.g., `βᵢ = Hash(rootᵢ₋₁)`). Because the hash output is unpredictable, the hash function acts as a "random oracle" that the Prover cannot game. This allows the Prover to generate the entire proof as a single string of data that can be verified by anyone at any time.

#### **3.4 Performance Considerations**

While powerful, FRI comes with a primary performance trade-off: **proof size**. The proof must contain Merkle paths for each query across multiple rounds, causing the proof size to scale polylogarithmically with the size of the computation. This is larger than the constant-size proofs of schemes like KZG. This trade-off between transparency and post-quantum security on one hand, and proof size on the other, is a central consideration in the design of modern ZKP systems.

---

### [Chapter 4: Algebraic Intermediate Representation (AIR) and Constraint Design](../4_air_constraints_design/README.md)

---

**Author:** [Okm165](https://github.com/Okm165) | [@bartolomeo_diaz](https://x.com/bartolomeo_diaz)
</file>

<file path="3_polynomial_commitment_scheme/src/main.rs">
//! # Educational FRI Protocol Implementation
//!
//! This code provides a simplified, educational implementation of the FRI (Fast Reed-Solomon
//! Interactive Oracle Proof of Proximity) protocol in Rust. It is designed for teaching
//! purposes to demonstrate the core concepts of FRI, which is a foundational component in
//! many modern STARK (Scalable Transparent Argument of Knowledge) systems.
//!
//! The implementation uses the `lambdaworks` library for finite field arithmetic, polynomials,
//! and Merkle trees.
//!
//! ## Protocol Flow Overview
//!
//! 1. **COMMIT**: The Prover evaluates a polynomial `P(x)` over a large domain (a Low-Degree
//!    Extension or LDE). It then commits to these evaluations using a Merkle tree.
//!
//! 2. **FOLD**: The Prover and Verifier engage in a recursive process. In each round:
//!     - The Verifier sends a random challenge, `beta`.
//!     - The Prover uses `beta` to "fold" the current set of evaluations into a smaller set,
//!       representing a new polynomial of half the degree.
//!     - The Prover commits to the new evaluations and the process repeats.
//!
//! 3. **LAST LAYER**: This folding continues until the polynomial is reduced to a constant. The
//!    Prover sends this constant value to the Verifier.
//!
//! 4. **QUERY**: The Verifier asks the Prover to reveal the evaluations of the polynomial at
//!    specific random points from the initial domain, along with their Merkle authentication paths
//!    for all layers.
//!
//! 5. **VERIFY**: The Verifier checks two things:
//!     - **Merkle Paths**: That the revealed evaluations are consistent with the commitments.
//!     - **Folding Consistency**: That the folding process was performed correctly at each step for
//!       the queried points. This ensures the Prover didn't cheat during the folding phase.
use lambdaworks_crypto::merkle_tree::backends::types::Keccak256Backend;
use lambdaworks_math::field::element::FieldElement;
use lambdaworks_math::field::fields::fft_friendly::babybear::Babybear31PrimeField;
use lambdaworks_math::polynomial::Polynomial;

use crate::prover::Prover;
use crate::types::FriParameters;
use crate::verifier::Verifier;

pub mod error;
pub mod prover;
pub mod types;
pub mod verifier;

/// The prime field for our computations (Babybear).
type F = Babybear31PrimeField;
/// A field element in the Babybear field.
type FE = FieldElement<F>;
/// The backend for our Merkle Tree, using Keccak256 for hashing.
type FriBackend = Keccak256Backend<F>;
/// The name of the protocol, used for initializing the transcript.
const PROTOCOL_ID: &[u8] = b"Educational FRI";

fn main() {
    // 1. SETUP
    // The polynomial we want to prove knowledge of: P(x) = x^3 - 3x + 2
    let poly = Polynomial::new(&[FE::from(2), -FE::from(3), FE::from(0), FE::from(1)]);
    let claimed_degree = 3;
    // Parameters: degree 3, blowup factor 4 (domain size 16), 2 queries.
    let params = FriParameters::new(claimed_degree, 8, 2);

    // 2. PROVE
    // The Prover generates a proof that it knows a polynomial of degree <= 3.
    let mut prover = Prover::new(poly, params.clone());
    let proof = prover.prove().unwrap();

    // 3. VERIFY
    // The Verifier checks the proof.
    let mut verifier = Verifier::new(params);
    match verifier.verify(&proof) {
        Ok(_) => println!("\n✅ SUCCESS: Proof verified successfully!"),
        Err(e) => println!("\n❌ FAILURE: Proof verification failed: {}", e),
    }
}
</file>

<file path="README.md">
# Zero-Knowledge Proof systems: A Deep Dive from Foundations to Frontiers

Welcome to a comprehensive course on the theory and practice of modern Zero-Knowledge Proof (ZKP) systems. This repository contains a series of lectures designed to deconstruct the complex world of ZKPs, starting from the mathematical first principles and building up to the architectural design of state-of-the-art systems.

---

## Course Lectures

The course is structured as a series of chapters, each building on the last.

- ### [Chapter 0: The Architecture of a Zero-Knowledge Proof System](./0_zkp_architecture/README.md)

  - **Description:** This chapter introduces the "big picture" of a modern ZKP system. We define the axiomatic guarantees of a proof system (Completeness, Soundness, Zero-Knowledge) and present the powerful architectural framework that separates the logic layer (PIOPs) from the cryptographic layer (PCSs).

- ### [Chapter 1: The Mathematical Toolkit for Verifiable Computation](./1_mathematical_toolkit/README.md)

  - **Description:** Here, we establish the three mathematical pillars of ZKPs: Finite Fields for perfect arithmetic, Polynomials for encoding computational logic (Arithmetization), and Cryptographic Hash Functions for building transparent and non-interactive systems.

- ### [Chapter 2: The Fast Fourier Transform for Polynomial Multiplication](./2_fast_polynomial_arithmetic/README.md)

  - **Description:** This chapter introduces the Fast Fourier Transform (FFT) as a high-performance algorithm for polynomial multiplication. We analyze the $`\Theta(n^2)`$ bottleneck of naive convolution and demonstrate how the FFT, by using the complex roots of unity, provides a $`\Theta(n \log n)`$ method for converting polynomials to and from the point-value representation, where multiplication is a linear-time operation. We derive the algorithm's structure, including the butterfly operation and bit-reversal permutation, and show how the same logic applies to the inverse transform.

- ### [Chapter 3: Foundations of Polynomial Commitment Schemes with a Focus on FRI](./3_polynomial_commitment_scheme/README.md)

  - **Description:** This chapter introduces Polynomial Commitment Schemes (PCS) as a foundational cryptographic primitive essential for modern Zero-Knowledge Proof (ZKP) systems. We will define a PCS and its core properties—binding, succinctness, and evaluation proofs—and explore its role in translating computational integrity claims into verifiable algebraic statements

- ### [Chapter 4: Algebraic Intermediate Representation (AIR) and Constraint Design](./4_air_constraints_design/README.md)
  - **Description:** This chapter introduces the Algebraic Intermediate Representation (AIR), the framework for converting computational claims into polynomial constraints. We detail the process of arithmetization by designing boundary and transition constraints, unifying them into a composition polynomial, and demonstrating how out-of-domain checks and the DEEP composition polynomial work together to ensure the integrity of a STARK proof.

---

## About & Contributions

Contributions, feedback, and questions are highly welcome. Please feel free to open an issue or a pull request to help improve the material for everyone.

**Author:** [Okm165](https://github.com/Okm165) | [@bartolomeo_diaz](https://x.com/bartolomeo_diaz)

## License

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).
</file>

<file path="0_zkp_architecture/README.md">
# Chapter 0: The Architecture of a Zero-Knowledge Proof System

**Abstract:** This chapter deconstructs modern Zero-Knowledge Proof (ZKP) systems into their constituent architectural components: Polynomial Interactive Oracle Proofs (PIOPs) and Polynomial Commitment Schemes (PCS). It establishes the formal guarantees of a ZKP and maps the prover's engineering pipeline, from a computational claim to a verifiable proof. The primary objective is to provide a robust framework for analyzing and comparing major ZKP systems like STARKs and SNARKs based on their core technical trade-offs.

**Learning Objectives:** Upon completion of this chapter, you will be able to:

1.  Define the three axiomatic guarantees of a ZKP system: Completeness, Soundness, and Zero-Knowledge.
2.  Explain the architectural framework, clearly differentiating the role of the PIOP (the logic layer) from the PCS (the cryptographic layer).
3.  Describe the prover's step-by-step pipeline from start to finish.
4.  Analyze and compare the fundamental trade-offs between prominent ZKP systems, particularly regarding trust models, proof size, and quantum resistance.

---

## Part 1: Foundational Principles of Verifiable Computation

#### The Core Problem: Verifiable Computation

The central problem that ZKP systems aim to solve can be stated formally.

**Formal Statement:** "Given a public relation $`R`$, how can a **Prover** convince a **Verifier** of the truth of a statement $`(x, w) \in R`$ (where $`x`$ is the public instance and $`w`$ is the secret witness), without the Verifier needing to re-execute the computation defined by $`R`$?"

#### The Three Axioms of a Proof System

Any valid Zero-Knowledge Proof system must satisfy three formal properties:

1.  **Completeness:** If the statement is true ($`(x, w) \in R`$), an honest Prover with witness $`w`$ can always produce a proof $`\pi`$ that an honest Verifier accepts.
2.  **Soundness:** If the statement is false ($`(x, w) \notin R`$), **no** computationally bounded, cheating Prover can produce a proof $`\pi`$ that the Verifier accepts, except with a negligible probability $`\varepsilon`$ (the soundness error).
3.  **Zero-Knowledge:** The proof $`\pi`$ reveals **no** information about the secret witness $`w`$. Formally, for any Verifier, there exists an efficient **Simulator** that, given only the public statement $`x`$, can generate a transcript indistinguishable from a real proof interaction.

#### The Modern ZKP "Compiler" Framework

Modern ZKPs are best understood as modular systems built by compiling an information-theoretic protocol with a cryptographic primitive. The pipeline is as follows:

`Computational Claim` → `Arithmetization` → `Polynomial IOP (PIOP)` → `Polynomial Commitment Scheme (PCS)` → `Final Proof`

---

## Part 2: The Architectural Blueprint: PIOPs and PCSs

The "compiler" framework consists of two primary components.

#### The "Logic Layer": Polynomial IOPs (PIOPs)

- **Definition:** A PIOP is an interactive protocol where a Prover convinces a Verifier of a statement by sending **polynomial oracles**. The Verifier probabilistically checks these oracles by making queries at randomly chosen points.
- **Purpose:** To reduce a complex computational claim (e.g., "This AIR is satisfiable") into a simpler, probabilistic claim about the low-degreeness of certain polynomials.
- **Security Basis:** **Information-Theoretic Soundness.** Security is derived from the algebraic properties of low-degree polynomials (e.g., the **Schwartz-Zippel Lemma**) and probability theory, not cryptographic assumptions.

#### The "Cryptographic Layer": Polynomial Commitment Schemes (PCS)

- **Definition:** A PCS is a cryptographic primitive that allows a Prover to commit to a polynomial $`P(x)`$ and later prove an evaluation $`P(z) = y`$ without revealing $`P(x)`$.
- **Core Steps:** $`\text{Setup}(\text{params})`$, $`\text{Commit}(P)`$, $`\text{Open}(P, z)`$, $`\text{VerifyEval}(C, z, y, \pi)`$.
- **Security Basis:** **Computational Hardness.** Security properties like **binding** (the inability to change the polynomial after commitment) and **hiding** (the inability to see the polynomial from the commitment) are derived from computationally hard problems, such as the Discrete Log Problem (DLP) or the collision-resistance of hash functions.

---

## Part 3: The Landscape and Trade-offs

#### The PCS Family: A Spectrum of Trade-offs

The choice of PCS is a critical design decision that dictates a system's core properties and engineering trade-offs.

| PCS     | Basis                             | Trust Model               | Advantages                              | Disadvantages                            |
| :------ | :-------------------------------- | :------------------------ | :-------------------------------------- | :--------------------------------------- |
| **KZG** | ECC Pairings ($`t\text{-SDH}`$)   | Trusted Setup (Universal) | Constant Size Proofs, Fast Verification | Not PQ-Secure, Requires Setup Ceremony   |
| **FRI** | Hash Functions (Collision-Resist) | Transparent               | PQ-Secure, Minimal Assumptions          | Larger Proofs, Slower Verifier           |
| **IPA** | ECC (Discrete Log)                | Transparent               | Transparent, Small Log-Size Proofs      | Slower Verifier, Not PQ-Secure (default) |

#### System Analysis: $`\text{PIOP} + \text{PCS} = \text{ZKP}`$

Combining a PIOP with a PCS yields a complete ZKP system. This table provides a high-level comparison of prominent ZKP systems based on their architectural choices.

| Scheme      | PIOP / Arithmetization     | PCS           | Trust Model                    | Proof Size $`O(\cdot)`$ | Quantum?          |
| :---------- | :------------------------- | :------------ | :----------------------------- | :---------------------- | :---------------- |
| **Groth16** | R1CS                       | Pairing-based | Circuit-Specific Trusted Setup | $`O(1)`$                | No                |
| **Plonk**   | Plonk-style (Custom Gates) | KZG           | Universal Trusted Setup        | $`O(1)`$                | No                |
| **STARK**   | AIR                        | FRI           | Transparent                    | $`O(\log^2 N)`$         | Yes (conjectured) |
| **Halo2**   | UltraPLONK (Lookups)       | IPA           | Transparent                    | $`O(\log N)`$           | No (default)      |
| **Plonky2** | Plonk-style                | FRI           | Transparent                    | $`O(\log^2 N)`$         | Yes (conjectured) |

#### Advanced Design: Univariate vs. Multivariate Systems

A key design axis is the type of polynomial used to represent the computation.

- **Univariate (Course Focus):** These systems use single-variable polynomials, such as $`P(x)`$. This approach involves "flattening" the execution trace into single columns of data. **Examples:** Classic STARK, Plonk, Marlin.
- **Multivariate:** These systems use polynomials in two or more variables, such as $`P(x, y)`$. This can offer a more natural or efficient representation for certain computational structures. **Examples:** HyperPlonk, Plonky2/Plonky3, and CircleSTARK.

---

### [Chapter 1: The Mathematical Toolkit for Verifiable Computation](../1_mathematical_toolkit/README.md)

---

**Author:** [Okm165](https://github.com/Okm165) | [@bartolomeo_diaz](https://x.com/bartolomeo_diaz)
</file>

<file path="1_mathematical_toolkit/README.md">
# Chapter 1: The Mathematical Toolkit for Verifiable Computation

**Abstract:** This chapter establishes the three mathematical pillars upon which modern ZKP systems are built: Finite Fields, which provide a domain for perfect and deterministic arithmetic; Polynomials, which serve as a universal language for encoding computational logic through a process called arithmetization; and Cryptographic Hash Functions, which provide the primitives for binding commitments and generating public randomness. The chapter explains how the algebraic properties of these tools, particularly the Schwartz-Zippel Lemma, enable efficient, probabilistic verification and form the foundation for transparent and secure proof systems.

**Learning Objectives:** Upon completion of this chapter, you will be able to:

1.  Explain why finite fields are necessary for verifiable computation, contrasting them with standard computer arithmetic.
2.  Define **Arithmetization** as the process of converting a computational statement into a system of polynomial equations over a finite field.
3.  Articulate how the **Schwartz-Zippel Lemma** enables efficient, probabilistic verification of polynomial identities.
4.  Describe the core security properties of cryptographic hash functions and their dual roles in building commitment schemes and achieving non-interactivity via the Fiat-Shamir heuristic.

---

## Part 1: Finite Fields – The Domain of Perfect Arithmetic

#### The Inadequacy of Standard Computer Arithmetic

The foundation of any verifiable system must be a system of arithmetic that is both deterministic and universally agreed upon. Standard computer arithmetic is unsuitable for cryptographic verification due to two critical flaws:

1.  **Integer Overflow:** Hardware-level integers use a fixed number of bits (e.g., 64-bit). Operations that exceed this capacity "wrap around," producing results that are mathematically incorrect but valid within the hardware's architecture. This behavior is implementation-dependent and cannot serve as the basis for a universal proof.
2.  **Floating-Point Imprecision:** Rational numbers (e.g., 1/3) and irrational numbers cannot be represented perfectly under standards like IEEE 754, leading to rounding errors. This means fundamental algebraic laws, such as associativity (`(a + b) + c = a + (b + c)`), may not hold precisely, making exact verification impossible.

To overcome these limitations, ZKP systems operate over **finite fields**, which provide a mathematical domain that is both finite and guarantees perfect arithmetic.

#### The Axiomatic Foundation of a Field

A finite field is a finite set of elements equipped with addition and multiplication operations that behave like standard arithmetic.

> **Definition: Prime Field**
>
> A prime field of order `p`, denoted $`F_p`$ or $`GF(p)`$, is the set of integers $\{0, 1, ..., p-1\}$ with addition and multiplication defined modulo `p`. For a set `F` to be a field, it must satisfy the following axioms for all `a, b, c` in `F`:
>
> - **Closure:** $`a + b`$ and $`a \cdot b`$ are in `F`.
> - **Associativity:** $`(a + b) + c = a + (b + c)`$ and $`(a \cdot b) \cdot c = a \cdot (b \cdot c)`$.
> - **Commutativity:** $`a + b = b + a`$ and $`a \cdot b = b \cdot a`$.
> - **Identities:** There exist unique elements $`0, 1 \in F`$ such that $`a + 0 = a`$ and $`a \cdot 1 = a`$.
> - **Inverses:** For every `a`, there exists $`-a`$ such that $`a + (-a) = 0`$. For every non-zero `a`, there exists a unique $`a^{-1}`$ such that $`a \cdot a^{-1} = 1`$.
> - **Distributivity:** $`a \cdot (b + c) = (a \cdot b) + (a \cdot c)`$.

The existence of a unique **multiplicative inverse** for every non-zero element is the most powerful property, as it enables division. This property is guaranteed if and only if the modulus `p` is a prime number.

#### Cryptographic Significance of Finite Fields

1.  **Deterministic Verification:** A prover and verifier operating over the same finite field will compute identical results, guaranteed. There are no rounding errors or implementation-specific ambiguities. This is the bedrock of verifiable computation.
2.  **Soundness from Vastness:** ZKP systems employ very large prime fields (e.g., with ~252-bit primes). The probability of a cheating prover succeeding by chance (e.g., finding a random point that coincidentally satisfies a false equation) is bounded by $`d/|F_p|`$, where `d` is the degree of a polynomial and $`|F_p|`$ is the field size. For a 252-bit prime, this probability is negligible, making the system computationally sound.

---

## Part 2: Polynomials – The Universal Language of Computation

#### Arithmetization: From Computation to Algebra

With a reliable number system established, we need a formal language to encode computational rules. **Arithmetization** is the process of converting a **Computational Integrity (CI) statement**—such as "this program executed correctly"—into an equivalent system of polynomial equations over a finite field.

To arithmetize a computation, we first represent its state over time as an **execution trace**. This is a table where each row represents the state of the machine at a discrete time step. For example, consider a Fibonacci-like sequence where $`a_{n+2} = a_{n+1} + a_n`$, starting with $`a_0 = 1`$ and $`a_1 = 1`$.

| Time Step (n) | State ($`a_n`$) |
| :------------ | :-------------- |
| 0             | 1               |
| 1             | 1               |
| 2             | 2               |
| 3             | 3               |

This trace represents the **witness**. The CI statement is: "This sequence of states was generated by the correct transition function, starting from the correct initial state." We convert this claim into polynomial constraints. The prover finds a low-degree polynomial $`P(x)`$ that interpolates the trace (i.e., $`P(n) = a_n`$ for each step `n`) and asserts that it satisfies two types of constraints:

1.  **Boundary Constraints:** Enforce the initial/final state. For our example:
    - $`P(0) = 1`$ and $`P(1) = 1`$.
2.  **Transition Constraints:** Enforce the state-update logic for all relevant steps.
    - $`P(x+2) - P(x+1) - P(x) = 0`$ must hold for $`x=0`$ and $`x=1`$.

The prover's task is to convince the verifier that such a low-degree polynomial $`P(x)`$ exists and satisfies these constraints, without revealing the polynomial itself.

#### The Schwartz-Zippel Lemma: The Engine of Probabilistic Verification

The security of this approach relies on the algebraic rigidity of low-degree polynomials, formalized by the **Fundamental Theorem of Algebra**: a non-zero univariate polynomial of degree `d` has at most `d` roots. The **Schwartz-Zippel Lemma** extends this idea to multivariate polynomials and provides a powerful tool for probabilistic checking.

> **The Lemma (Formal):** Let $`P(x_1, ..., x_m)`$ be a non-zero multivariate polynomial of total degree `d` over a field `F`. Let `S` be a finite, non-empty subset of `F`. If values $`r_1, ..., r_m`$ are chosen independently and uniformly at random from `S`, then: $`\text{Pr}[P(r_1, ..., r_m) = 0] \le \frac{d}{|S|}`$

This lemma allows a Verifier to check a Prover's claim that a large, complex constraint polynomial is identically zero. Instead of evaluating it everywhere, the Verifier requests an evaluation at a single random point. If the result is zero, the Verifier can be highly confident that the polynomial is the zero polynomial everywhere, as the probability of accidentally hitting a root of a non-zero polynomial is negligibly small in a large field.

---

## Part 3: Cryptographic Primitives for Practical Systems

#### The Commitment Problem

After arithmetization, the Prover must commit to the witness polynomials, fixing them immutably before the Verifier's random challenges are known. This is achieved with a **Polynomial Commitment Scheme (PCS)**, which was introduced in Chapter 1. The security of many transparent PCSs, like FRI, is built upon cryptographic hash functions. A commitment scheme must be:

- **Binding:** Once committed, the Prover cannot open the commitment to a different polynomial.
- **Hiding:** The commitment reveals no information about the secret polynomial.

#### Cryptographic Hash Functions: The Digital Fingerprint

A cryptographic hash function $`H`$ is a deterministic public function that maps an arbitrary-length input to a fixed-length output (a _digest_). They are the fundamental building block for transparency and non-interactivity.

**Key Security Properties:**

1.  **Preimage Resistance (One-Way):** Given a hash output `y`, it is computationally infeasible to find an input `x` such that $`H(x) = y`$.
2.  **Second Preimage Resistance:** Given an input `x`, it is infeasible to find a different input $`x' \ne x`$ such that $`H(x) = H(x')`$.
3.  **Collision Resistance:** It is infeasible to find _any_ two distinct inputs $`x, x'`$ such that $`H(x) = H(x')`$.

#### Applications in ZKP Systems

Hash functions serve two critical roles in modern ZKPs like STARKs:

1.  **Building Transparent Commitments (Merkle Trees):** The FRI protocol uses hash functions to build Merkle Trees. By hashing polynomial evaluation data and then recursively hashing the resulting digests, the Prover produces a single root hash. This hash serves as a binding commitment to the entire set of evaluations. This approach is **transparent** because it relies only on the public hash function and requires no trusted setup or secret parameters. Its security against quantum computers is conjectured to be strong, as breaking it requires finding collisions, a problem for which quantum algorithms offer only a polynomial speedup.

2.  **Achieving Non-Interactivity (The Fiat-Shamir Heuristic):** Interactive proofs require a Verifier to provide random challenges. To create a non-interactive proof that can be publicly verified, the **Fiat-Shamir heuristic** replaces the Verifier with a hash function. The Prover generates challenges by hashing the public transcript of the proof up to that point: $`c = H(\text{public\_inputs} \mathbin{\|} \text{prover\_message\_1} \mathbin{\|} \dots)`$. In the security analysis, the hash function is modeled as a **Random Oracle**—a perfect, unpredictable source of randomness that the Prover cannot manipulate to generate favorable challenges.

---

### [Chapter 2: The Fast Fourier Transform for Polynomial Multiplication](../2_fast_polynomial_arithmetic/README.md)

---

**Author:** [Okm165](https://github.com/Okm165) | [@bartolomeo_diaz](https://x.com/bartolomeo_diaz)
</file>

<file path="4_air_constraints_design/src/main.rs">
use lambdaworks_math::field::element::FieldElement;
use lambdaworks_math::field::fields::fft_friendly::babybear_u32::Babybear31PrimeField;

use crate::arithmetization::Arithmetization;
use crate::composition::Composition;
use crate::deep_composition::DeepComposition;
use crate::trace::generate_fibonacci_trace;

pub mod arithmetization;
pub mod composition;
pub mod deep_composition;
pub mod trace;

/// The prime field for our computations (Babybear).
type F = Babybear31PrimeField;
/// A field element in the Babybear field.
type FE = FieldElement<F>;

/// This demo walks through the main algebraic steps of a STARK proving system,
/// from the initial computation to the final consistency checks. It omits the
/// cryptographic commitment scheme (Merkle Trees) and the FRI protocol itself,
/// focusing instead on the design of the polynomial constraints.
fn main() {
    println!("--- STARK Polynomial IOP Demo: Fibonacci Sequence ---");

    // ============================================================================
    // 1. SETUP & TRACE GENERATION
    // ============================================================================
    println!("\n-- STEP 1: EXECUTION TRACE ------------------------------------");
    let trace_length = 8;
    let blowup_factor = 8;
    let fib_trace = generate_fibonacci_trace(trace_length);
    println!("The Prover executes the computation and records the trace.");
    println!(
        "  Fibonacci trace (len {}): {:?}",
        trace_length,
        fib_trace
            .iter()
            .map(|e| e.representative())
            .collect::<Vec<_>>()
    );

    // ============================================================================
    // 2. PROVER: ARITHMETIZATION
    // ============================================================================
    let arithmetization = Arithmetization::new(&fib_trace, blowup_factor);

    // ============================================================================
    // 3. PROVER->VERIFIER INTERACTION
    // ============================================================================
    // Verifier sends random challenges to the Prover to ensure security.

    // <-- Verifier sends challenges α₁, α₂ for the composition polynomial.
    let alpha1 = FE::from(5);
    let alpha2 = FE::from(7);
    println!(
        "\n<-- Verifier to Prover: Send challenges α₁={}, α₂={}",
        alpha1.representative(),
        alpha2.representative()
    );
    let composition = Composition::new(&arithmetization, &alpha1, &alpha2);

    // <-- Verifier sends a random out-of-domain point 'z'.
    let z = FE::from(10);
    println!(
        "\n<-- Verifier to Prover: Send out-of-domain point z={}",
        z.representative()
    );
    composition.perform_ood_check(&arithmetization, &alpha1, &alpha2, &z);

    // <-- Verifier sends challenges β's for the DEEP polynomial.
    let betas = [FE::from(11), FE::from(13), FE::from(15), FE::from(17)];
    println!("\n<-- Verifier to Prover: Send challenges β's for DEEP polynomial");
    let deep_composition = DeepComposition::new(&arithmetization, &composition, &z, &betas);

    // ============================================================================
    // 4. FINAL VERIFICATION (Simulating a FRI query result)
    // ============================================================================
    // In a real system, the FRI protocol would conclude by querying a few points.
    // We simulate one such query and the final check.
    let x0_index = 5; // A random index into the LDE domain.
    deep_composition.perform_final_spot_check(&arithmetization, &composition, &z, &betas, x0_index);

    println!("\n\n--- Proof Verified Successfully ---");
}
</file>

<file path="Cargo.toml">
[workspace]
members = [
    "0_zkp_architecture",
    "1_mathematical_toolkit",
    "2_fast_polynomial_arithmetic", "3_polynomial_commitment_scheme", "4_air_constraints_design",
]
resolver = "3"

[workspace.package]
edition = "2021"
version = "1.0.0"
readme = "README.md"

[workspace.dependencies]
criterion = { version = "0.5", features = ["html_reports"] }
lambdaworks-crypto = { git = "https://github.com/lambdaclass/lambdaworks", features = ["std"] }
lambdaworks-math = { git = "https://github.com/lambdaclass/lambdaworks", features = ["std"] }
proptest = "1.2.0"
hex = "0.4.3"
</file>

</files>
